# -*- coding: utf-8 -*-
"""5210_Final_Project_(2) (1).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1a5HdHX5PmxrdivINoyOLOdWYk7sudIiT

Check DAM Dataset Structure
"""

import pandas as pd

# Check the structure of the 2019 file
xlsx_file = "DAM_2019.xlsx"
xls = pd.ExcelFile(xlsx_file)

print("="*60)
print("Checking DAM_2019.xlsx Structure")
print("="*60)

# List all sheets
print(f"\nAll Sheets: {xls.sheet_names}\n")

# Read the first sheet (usually Jan)
first_sheet = xls.sheet_names[0]
df = pd.read_excel(xlsx_file, sheet_name=first_sheet)

print(f"Sheet Name: {first_sheet}")
print(f"Shape: {df.shape}")
print("\n" + "="*60)
print("First 5 Rows of Data:")
print("="*60)
print(df.head())

print("\n" + "="*60)
print("Column List:")
print("="*60)
for i, col in enumerate(df.columns):
    print(f"{i}: '{col}'")

print("\n" + "="*60)
print("Data Types:")
print("="*60)
print(df.dtypes)

print("\n" + "="*60)
print("Unique Settlement Point Values (if column exists):")
print("="*60)
# Try to find settlement point related columns
settlement_cols = [col for col in df.columns if 'settlement' in col.lower() or 'point' in col.lower()]
if settlement_cols:
    print(f"Found relevant columns: {settlement_cols}")
    for col in settlement_cols:
        unique_vals = df[col].unique()
        print(f"\nUnique values for {col} (first 20):\n")
        print(unique_vals[:20])
else:
    print("No column names containing 'settlement' or 'point' found")

"""
MODULE 1.1: ERCOT DAM Price Processing
"""
import pandas as pd
import numpy as np
from pathlib import Path
from datetime import datetime, timedelta
import warnings
warnings.filterwarnings('ignore')

# Configuration
YEARS = [2019, 2020, 2021, 2022, 2023]
ZONES = ['LZ_HOUSTON', 'LZ_WEST']
ZONE_LABELS = {'LZ_HOUSTON': 'HOUSTON', 'LZ_WEST': 'WEST'}
MONTH_NAMES = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun',
               'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']

OUT_DIR = Path('out')
OUT_DIR.mkdir(exist_ok=True)


def get_expected_hours(year):
    """Returns expected hours for a year accounting for DST and leap years."""
    is_leap = (year % 4 == 0) and (year % 100 != 0 or year % 400 == 0)
    base_hours = 8784 if is_leap else 8760
    # DST: Spring loses 1 hour, Fall gains 1 hour -> net effect is 0
    return base_hours


def parse_hour_ending(row):
    """Converts ERCOT Hour Ending format to Hour-Beginning datetime."""
    date_str = row['Delivery Date']
    hour_ending_str = row['Hour Ending']

    if isinstance(date_str, str):
        date = pd.to_datetime(date_str, format='%m/%d/%Y')
    else:
        date = pd.to_datetime(date_str)

    hour_ending = int(hour_ending_str.split(':')[0])
    hour_beginning = hour_ending - 1

    return date + timedelta(hours=hour_beginning)


def load_dam_data(years=YEARS, zones=ZONES):
    """Load DAM data preserving all actual trading hours."""

    all_data = []

    for year in years:
        xlsx_file = f"DAM_{year}.xlsx"

        if not Path(xlsx_file).exists():
            print(f"Warning: {xlsx_file} not found, skipping")
            continue

        xls = pd.ExcelFile(xlsx_file)
        year_data = []

        for month_name in MONTH_NAMES:
            if month_name not in xls.sheet_names:
                continue

            df_month = pd.read_excel(xlsx_file, sheet_name=month_name)
            df_filtered = df_month[df_month['Settlement Point'].isin(zones)].copy()

            if not df_filtered.empty:
                year_data.append(df_filtered)

        if year_data:
            df_year = pd.concat(year_data, ignore_index=True)
            df_year['Year'] = year
            all_data.append(df_year)
            print(f"Loaded {year}: {len(df_year)} rows")

    df = pd.concat(all_data, ignore_index=True)

    # Parse datetime (Hour-Beginning)
    df['datetime'] = df.apply(parse_hour_ending, axis=1)

    # Mark DST repeated hours
    df['is_dst_repeated'] = df['Repeated Hour Flag'] == 'Y'

    # Create unique hour ID (append '_R' for repeated hours)
    df['hour_id'] = df.apply(
        lambda row: f"{row['datetime'].strftime('%Y-%m-%d_%H')}" +
                    ('_R' if row['is_dst_repeated'] else ''),
        axis=1
    )

    # Rename columns
    df = df.rename(columns={
        'Settlement Point': 'zone_ercot',
        'Settlement Point Price': 'price_usd_per_mwh'
    })

    # Add zone label
    df['zone'] = df['zone_ercot'].map(ZONE_LABELS)

    # Select columns
    df = df[['Year', 'datetime', 'hour_id', 'zone', 'zone_ercot',
             'price_usd_per_mwh', 'is_dst_repeated']]

    return df


def validate_data(df):
    """Validate loaded data for completeness."""

    print("\n" + "=" * 60)
    print("Data Validation")
    print("=" * 60)

    for year in sorted(df['Year'].unique()):
        df_year = df[df['Year'] == year]
        expected = get_expected_hours(year)

        print(f"\nYear {year} (expected {expected} hours/zone):")

        for zone in df_year['zone'].unique():
            df_zone = df_year[df_year['zone'] == zone]
            actual = len(df_zone)
            repeated = df_zone['is_dst_repeated'].sum()
            status = "OK" if actual == expected else "CHECK"

            print(f"  {zone}: {actual} hours (repeated={repeated}) [{status}]")

    # Price statistics
    print(f"\nPrice Statistics:")
    print(f"  Min:  ${df['price_usd_per_mwh'].min():.2f}/MWh")
    print(f"  Max:  ${df['price_usd_per_mwh'].max():.2f}/MWh")
    print(f"  Mean: ${df['price_usd_per_mwh'].mean():.2f}/MWh")
    print(f"  Negative hours: {(df['price_usd_per_mwh'] < 0).sum()}")


def add_analysis_flags(df):
    """Add flags for extreme prices and Uri period."""

    df = df.copy()

    # Extreme price flags
    df['is_extreme_high'] = df['price_usd_per_mwh'] > 1000
    df['is_extreme_low'] = df['price_usd_per_mwh'] < -100
    df['is_extreme'] = df['is_extreme_high'] | df['is_extreme_low']

    # Winter Storm Uri flag (Feb 12-20, 2021)
    df['is_uri_period'] = (
        (df['Year'] == 2021) &
        (df['datetime'] >= pd.Timestamp('2021-02-12')) &
        (df['datetime'] <= pd.Timestamp('2021-02-20 23:59:59'))
    )

    # Capped price for sensitivity analysis
    df['price_capped_usd_per_mwh'] = df['price_usd_per_mwh'].clip(-100, 1000)

    return df


def save_by_zone_year(df, out_dir=OUT_DIR):
    """Save data as parquet files by zone and year."""

    print("\n" + "=" * 60)
    print("Saving Files")
    print("=" * 60)

    for year in sorted(df['Year'].unique()):
        for zone in df['zone'].unique():
            df_subset = df[(df['Year'] == year) & (df['zone'] == zone)].copy()

            if len(df_subset) == 0:
                continue

            # Sort by datetime and hour_id (to keep repeated hours in order)
            df_subset = df_subset.sort_values(['datetime', 'hour_id']).reset_index(drop=True)

            filename = f"ercot_dam_{zone}_{year}.parquet"
            df_subset.to_parquet(out_dir / filename, index=False)
            print(f"  Saved: {filename} ({len(df_subset)} hours)")


# Main Execution
print("=" * 60)
print("MODULE 1.1: ERCOT DAM Price Processing")
print("Preserving all actual trading hours")
print("=" * 60)

# Load data
dam_df = load_dam_data()

# Validate
validate_data(dam_df)

# Add analysis flags
dam_df = add_analysis_flags(dam_df)

# Save
save_by_zone_year(dam_df)

# Summary by year
print("\n" + "=" * 60)
print("Summary Statistics by Year")
print("=" * 60)

summary = dam_df.groupby(['Year', 'zone']).agg({
    'price_usd_per_mwh': ['mean', 'min', 'max', 'std'],
    'is_extreme': 'sum',
    'is_uri_period': 'sum'
}).round(2)

summary.columns = ['Avg_Price', 'Min_Price', 'Max_Price', 'Std_Price',
                   'Extreme_Hours', 'Uri_Hours']
print(summary)

# Uri analysis for 2021
if 2021 in dam_df['Year'].values:
    print("\n" + "=" * 60)
    print("Winter Storm Uri Analysis (Feb 12-20, 2021)")
    print("=" * 60)

    uri_data = dam_df[dam_df['is_uri_period']]
    for zone in uri_data['zone'].unique():
        zone_uri = uri_data[uri_data['zone'] == zone]
        print(f"\n  {zone}:")
        print(f"    Hours: {len(zone_uri)}")
        print(f"    Avg Price: ${zone_uri['price_usd_per_mwh'].mean():.2f}/MWh")
        print(f"    Max Price: ${zone_uri['price_usd_per_mwh'].max():.2f}/MWh")
        print(f"    Hours > $1000: {(zone_uri['price_usd_per_mwh'] > 1000).sum()}")

print("\nMODULE 1.1 COMPLETE.")
print("Data preserves all actual trading hours including DST repeated hours")

"""Module_1_2_Gas_Prices"""

"""
MODULE 1.2: Texas Natural Gas Price Processing
Uses EIA Texas Electric Power gas prices (actual prices paid by power plants)

Input: Texas_Natural_Gas_Price_Sold_to_Electric_Power_Consumers.csv
Output: gas_prices_hourly_{year}.parquet
"""
import pandas as pd
import numpy as np
from pathlib import Path
import warnings
warnings.filterwarnings('ignore')

# Configuration
YEARS = [2019,2020,2021,2022,2023]
OUT_DIR = Path('out')
OUT_DIR.mkdir(exist_ok=True)


def get_expected_hours(year):
    """Calculate expected hours for a year."""
    is_leap = (year % 4 == 0) and (year % 100 != 0 or year % 400 == 0)
    return 8784 if is_leap else 8760


print("=" * 70)
print("MODULE 1.2: Texas Natural Gas Price Processing")
print("Data: EIA Texas Electric Power Consumer Prices")
print("=" * 70)

# Step 1: Load data
print("\nStep 1: Loading Texas Electric Power gas prices...")

df_raw = pd.read_csv(
    'Texas_Natural_Gas_Price_Sold_to_Electric_Power_Consumers.csv',
    skiprows=4
)
df_raw.columns = ['month_year', 'price_usd_mcf']

# Convert $/MCF to $/MMBtu (1 MCF ≈ 1.037 MMBtu, but often treated as 1:1)
# EIA uses MCF (thousand cubic feet), industry often uses MMBtu
# For natural gas: 1 MCF ≈ 1.037 MMBtu, we'll use 1:1 for simplicity
df_raw['price_usd_mmbtu'] = df_raw['price_usd_mcf']

# Parse date
df_raw['date'] = pd.to_datetime(df_raw['month_year'], format='%b %Y')
df_raw['year'] = df_raw['date'].dt.year
df_raw['month'] = df_raw['date'].dt.month

# Filter to study years
df_gas = df_raw[df_raw['year'].isin(YEARS)].copy()
df_gas = df_gas.sort_values('date').reset_index(drop=True)

print(f"Loaded {len(df_gas)} months for years {YEARS}")

# Step 2: Display monthly prices
print("\n" + "=" * 70)
print("Monthly Gas Prices - Texas Electric Power ($/MMBtu)")
print("=" * 70)

for year in YEARS:
    year_data = df_gas[df_gas['year'] == year].sort_values('month')
    print(f"\n{year}:")
    for _, row in year_data.iterrows():
        month_name = row['date'].strftime('%b')
        price = row['price_usd_mmbtu']
        flag = " <<<< WINTER STORM URI" if (year == 2021 and row['month'] == 2) else ""
        print(f"  {month_name}: ${price:.2f}{flag}")

# Step 3: Compare with Henry Hub (for reference)
print("\n" + "=" * 70)
print("Key Insight: Texas vs Henry Hub during Uri")
print("=" * 70)
print("""
                    Texas Electric Power    Henry Hub (Louisiana)
Feb 2021 (Uri):          $61.88/MMBtu           ~$5.35/MMBtu
                              │                      │
                              └──── 11.6x higher ────┘

This difference reflects:
- Pipeline constraints during the freeze
- Local supply shortage in Texas
- Actual costs faced by Texas power plants
""")

# Step 4: Expand to hourly
print("=" * 70)
print("Step 2: Expanding monthly to hourly data...")
print("=" * 70)

all_hourly = {}

for year in YEARS:
    year_data = df_gas[df_gas['year'] == year]
    hourly_records = []

    for _, row in year_data.iterrows():
        month = int(row['month'])
        price = float(row['price_usd_mmbtu'])

        # Generate hours for this month
        month_start = pd.Timestamp(f'{year}-{month:02d}-01')
        if month == 12:
            month_end = pd.Timestamp(f'{year+1}-01-01')
        else:
            month_end = pd.Timestamp(f'{year}-{month+1:02d}-01')

        hours = pd.date_range(start=month_start, end=month_end,
                             freq='h', inclusive='left')

        for hour in hours:
            hourly_records.append({
                'datetime': hour,
                'gas_price_usd_mmbtu': price,
                'is_uri_month': (year == 2021 and month == 2)
            })

    df_hourly = pd.DataFrame(hourly_records)

    # Validate
    expected = get_expected_hours(year)
    actual = len(df_hourly)
    status = "OK" if actual == expected else "CHECK"

    print(f"\n{year}: {actual} hours (expected {expected}) [{status}]")
    print(f"  Price range: ${df_hourly['gas_price_usd_mmbtu'].min():.2f} - "
          f"${df_hourly['gas_price_usd_mmbtu'].max():.2f}/MMBtu")

    if year == 2021:
        uri_hours = df_hourly[df_hourly['is_uri_month']]
        print(f"  Feb 2021 (Uri month): {len(uri_hours)} hours @ ${uri_hours['gas_price_usd_mmbtu'].iloc[0]:.2f}/MMBtu")

    # Save
    filename = f"gas_prices_hourly_{year}.parquet"
    df_hourly.to_parquet(OUT_DIR / filename, index=False)
    print(f"  Saved: {filename}")

    all_hourly[year] = df_hourly

# Step 5: Summary
print("\n" + "=" * 70)
print("Annual Summary")
print("=" * 70)

summary_data = []
for year in YEARS:
    df = all_hourly[year]
    summary_data.append({
        'Year': year,
        'Avg_Price': df['gas_price_usd_mmbtu'].mean(),
        'Min_Price': df['gas_price_usd_mmbtu'].min(),
        'Max_Price': df['gas_price_usd_mmbtu'].max(),
    })

df_summary = pd.DataFrame(summary_data)
print(df_summary.round(2).to_string(index=False))

# Save monthly reference
df_gas[['year', 'month', 'price_usd_mmbtu']].to_csv(
    OUT_DIR / 'texas_gas_prices_monthly.csv', index=False
)

print("\n" + "=" * 70)
print("MODULE 1.2 COMPLETE!")
print("=" * 70)
print("Using Texas Electric Power prices (not Henry Hub)")
print("Feb 2021 Uri impact properly captured at $61.88/MMBtu")

"""Module 1.3: PV Capacity Data Processing"""

"""
MODULE 1.3: PV Output Processing
Load PVWatts TMY data and expand to analysis years

Input: HOUSTON_pvwatts_hourly_cleaned.csv, WEST_pvwatts_hourly_cleaned.csv
Output: pv_output_{zone}_{year}.parquet

"""
import pandas as pd
import numpy as np
from pathlib import Path
import warnings
warnings.filterwarnings('ignore')

# Configuration
YEARS = [2019, 2020, 2021, 2022, 2023]
ZONES = ['HOUSTON', 'WEST']
ZONE_FILES = {
    'HOUSTON': 'HOUSTON_pvwatts_hourly_cleaned.csv',  # Note: typo in original filename
    'WEST': 'WEST_pvwatts_hourly_cleaned.csv'
}

OUT_DIR = Path('out')
OUT_DIR.mkdir(exist_ok=True)


def get_expected_hours(year):
    """Calculate expected hours for a year."""
    is_leap = (year % 4 == 0) and (year % 100 != 0 or year % 400 == 0)
    return 8784 if is_leap else 8760


def load_tmy_data(filepath):
    """Load and clean PVWatts TMY data."""
    df = pd.read_csv(filepath, encoding='utf-8-sig')
    df.columns = df.columns.str.strip()

    # Remove header rows if duplicated
    df = df[df['Month'] != 'Month'].copy()

    # Convert to numeric
    for col in ['Month', 'Day', 'Hour', 'AC System Output (MW)']:
        df[col] = pd.to_numeric(df[col], errors='coerce')

    df = df.dropna().reset_index(drop=True)

    # Rename for clarity
    df = df.rename(columns={'AC System Output (MW)': 'ac_mw'})

    return df[['Month', 'Day', 'Hour', 'ac_mw']]


def expand_tmy_to_year(df_tmy, year):
    """Expand TMY data to a specific year, handling leap years."""

    is_leap = (year % 4 == 0) and (year % 100 != 0 or year % 400 == 0)
    expected_hours = 8784 if is_leap else 8760

    records = []

    for _, row in df_tmy.iterrows():
        month = int(row['Month'])
        day = int(row['Day'])
        hour = int(row['Hour'])
        ac_mw = float(row['ac_mw'])

        # Skip Feb 29 in TMY (if present) - we'll handle leap year separately
        if month == 2 and day == 29:
            continue

        # Create datetime (Hour in PVWatts is 0-23, Hour-Beginning format)
        try:
            dt = pd.Timestamp(year=year, month=month, day=day, hour=hour)
            records.append({
                'datetime': dt,
                'ac_mw': ac_mw
            })
        except ValueError:
            # Skip invalid dates
            continue

    # For leap years, duplicate Feb 28 data for Feb 29
    if is_leap:
        feb28_data = [r for r in records
                      if r['datetime'].month == 2 and r['datetime'].day == 28]
        for r in feb28_data:
            records.append({
                'datetime': r['datetime'] + pd.Timedelta(days=1),
                'ac_mw': r['ac_mw']
            })

    df = pd.DataFrame(records)
    df = df.sort_values('datetime').reset_index(drop=True)

    return df


print("=" * 70)
print("MODULE 1.3: PV Output Processing")
print("=" * 70)

all_results = {}

for zone in ZONES:
    filename = ZONE_FILES[zone]

    print(f"\n{'=' * 70}")
    print(f"Processing Zone: {zone}")
    print(f"{'=' * 70}")

    try:
        # Load TMY data
        print(f"Loading {filename}...")
        df_tmy = load_tmy_data(filename)

        print(f"  TMY data loaded: {len(df_tmy)} hours")
        print(f"  Output range: {df_tmy['ac_mw'].min():.2f} - {df_tmy['ac_mw'].max():.2f} MW")

        # Capacity factor (assuming 100 MW system)
        annual_mwh = df_tmy['ac_mw'].sum()
        cf = (annual_mwh / (100 * 8760)) * 100
        print(f"  TMY capacity factor: {cf:.1f}%")

        # Process each year
        for year in YEARS:
            print(f"\n  Year {year}...")

            df_year = expand_tmy_to_year(df_tmy, year)
            expected = get_expected_hours(year)
            actual = len(df_year)

            status = "OK" if actual == expected else "CHECK"
            print(f"    Hours: {actual} (expected {expected}) [{status}]")

            # Statistics
            total_mwh = df_year['ac_mw'].sum()
            cf_year = (total_mwh / (100 * actual)) * 100
            print(f"    Annual generation: {total_mwh:,.0f} MWh")
            print(f"    Capacity factor: {cf_year:.1f}%")
            print(f"    Peak output: {df_year['ac_mw'].max():.2f} MW")

            # Save
            output_file = OUT_DIR / f"pv_output_{zone}_{year}.parquet"
            df_year.to_parquet(output_file, index=False)
            print(f"    Saved: {output_file.name}")

            all_results[(zone, year)] = df_year

    except FileNotFoundError:
        print(f"  ERROR: {filename} not found!")
        continue
    except Exception as e:
        print(f"  ERROR: {str(e)}")
        import traceback
        traceback.print_exc()
        continue

# Summary
print("\n" + "=" * 70)
print("SUMMARY")
print("=" * 70)

if all_results:
    summary_rows = []
    for (zone, year), df in all_results.items():
        summary_rows.append({
            'Zone': zone,
            'Year': year,
            'Hours': len(df),
            'Annual_MWh': round(df['ac_mw'].sum(), 0),
            'Peak_MW': round(df['ac_mw'].max(), 2),
            'CF_%': round((df['ac_mw'].sum() / (100 * len(df))) * 100, 1)
        })

    df_summary = pd.DataFrame(summary_rows)
    print(df_summary.to_string(index=False))

    df_summary.to_csv(OUT_DIR / 'pv_output_summary.csv', index=False)

print("\nMODULE 1.3 COMPLETE.")

"""Prepare for Module 2"""

"""
Configuration file conversion and validation
Converts uploaded Excel/CSV files to standard format
"""

import pandas as pd
from pathlib import Path

OUT_DIR = Path('out')
OUT_DIR.mkdir(exist_ok=True)

print("="*70)
print("Configuration Files Processing")
print("="*70)

# 1. Tech Params
print("\n1. Processing tech_params...")
df_tech_raw = pd.read_excel('1.1 tech_params .xlsx')

tech_rows = []

# Solar PV
solar_pv = df_tech_raw[df_tech_raw['tech'] == 'Solar_PV'].iloc[0]
tech_rows.append({
    'tech': 'Solar_PV',
    'capex_per_kw': solar_pv['capex_per_kw'],
    'fom_per_kw_yr': solar_pv['fom_per_kw_yr'],
    'vom_per_mwh': solar_pv['vom_per_mwh'],
    'lifetime_years': int(solar_pv['lifetime_years']),
    'wacc': solar_pv['wacc_base'],
    'heat_rate_mmbtu_per_mwh': None,
    'notes': 'Lazard 2025 utility PV'
})

# Solar+Storage
ss = df_tech_raw[df_tech_raw['tech'] == 'Solar_PV_Storage'].iloc[0]
pv_capex = solar_pv['capex_per_kw']
pv_fom = solar_pv['fom_per_kw_yr']
battery_power_capex = ss['storage_power_capex_per_kw']
battery_energy_capex = ss['storage_energy_capex_per_kwh']
battery_fom = ss['battery_fom_per_kw_yr']

for duration_h in [2, 4]:
    total_capex = pv_capex + battery_power_capex + (battery_energy_capex * duration_h)
    total_fom = pv_fom + battery_fom

    tech_rows.append({
        'tech': f'Solar_PV_Storage_{duration_h}h',
        'capex_per_kw': round(total_capex, 2),
        'fom_per_kw_yr': round(total_fom, 2),
        'vom_per_mwh': 0,
        'lifetime_years': 30,
        'wacc': ss['wacc_base'],
        'heat_rate_mmbtu_per_mwh': None,
        'notes': f'PV + {duration_h}h battery'
    })

# CCGT
ccgt = df_tech_raw[df_tech_raw['tech'] == 'CCGT'].iloc[0]
tech_rows.append({
    'tech': 'CCGT',
    'capex_per_kw': ccgt['capex_per_kw'],
    'fom_per_kw_yr': ccgt['fom_per_kw_yr'],
    'vom_per_mwh': ccgt['vom_per_mwh'],
    'lifetime_years': int(ccgt['lifetime_years']),
    'wacc': ccgt['wacc_base'],
    'heat_rate_mmbtu_per_mwh': ccgt['heat_rate_mmbtu_per_mwh'],
    'notes': 'Lazard 2025 CCGT'
})

df_tech = pd.DataFrame(tech_rows)
df_tech.to_csv(OUT_DIR / 'tech_params.csv', index=False)
print("Saved: tech_params.csv")
print(df_tech.to_string(index=False))

# 2. Policy Params
print("\n" + "="*70)
print("2. Processing policy_params...")
df_policy_raw = pd.read_excel('1.2 policy_params_from_epa.xlsx')

epa_row = df_policy_raw.iloc[0]
co2_t_per_mmbtu = epa_row['gas_co2_factor_t_per_mmbtu']
co2_t_per_mwh_ccgt = epa_row['co2_t_per_mwh_ccgt']

print(f"EPA CO2 emission factor: {co2_t_per_mmbtu:.5f} tCO2/MMBtu")
print(f"CCGT emissions: {co2_t_per_mwh_ccgt:.6f} tCO2/MWh")

# Texas Electric Power prices (from Module 1.2)
gas_prices_yearly = {
    2019: 2.43,
    2020: 2.15,
    2021: 9.52,
    2022: 6.34,
    2023: 2.58
}

policy_rows = []

for year, gas_price in gas_prices_yearly.items():
    policy_rows.append({
        'scenario': 'historical',
        'year': year,
        'gas_price_usd_per_mmbtu': gas_price,
        'carbon_price_usd_per_t': 0,
        'co2_t_per_mmbtu': co2_t_per_mmbtu,
        'co2_t_per_mwh_ccgt': co2_t_per_mwh_ccgt,
        'notes': f'EIA Texas Electric Power {year} avg'
    })

# Carbon sensitivity scenarios
for carbon_price in [50, 100]:
    policy_rows.append({
        'scenario': f'carbon_{carbon_price}',
        'year': 'all',
        'gas_price_usd_per_mmbtu': 'from_hourly',
        'carbon_price_usd_per_t': carbon_price,
        'co2_t_per_mmbtu': co2_t_per_mmbtu,
        'co2_t_per_mwh_ccgt': co2_t_per_mwh_ccgt,
        'notes': f'Carbon sensitivity ${carbon_price}/tCO2'
    })

df_policy = pd.DataFrame(policy_rows)
df_policy.to_csv(OUT_DIR / 'policy_params.csv', index=False)
print("\nSaved: policy_params.csv")
print(df_policy.to_string(index=False))

# 3. Battery Config
print("\n" + "="*70)
print("3. Creating battery_config...")

battery_rows = []
for duration_h in [2, 4]:
    battery_rows.append({
        'duration_h': duration_h,
        'rt_efficiency': 0.85,
        'power_capacity_mw': 100,
        'energy_capacity_mwh': 100 * duration_h,
        'energy_capex_per_kwh': 250,
        'fom_per_kw_yr': 20,
        'notes': f'{duration_h}-hour duration battery'
    })

df_battery = pd.DataFrame(battery_rows)
df_battery.to_csv(OUT_DIR / 'battery_config.csv', index=False)
print("\nSaved: battery_config.csv")
print(df_battery.to_string(index=False))

# Summary
print("\n" + "="*70)
print("SUMMARY")
print("="*70)
print("\nConfiguration files created:")
print("  1. tech_params.csv - 4 technologies (PV, PV+2h, PV+4h, CCGT)")
print("  2. policy_params.csv - 7 scenarios (5 historical + 2 carbon)")
print("  3. battery_config.csv - 2 durations (2h, 4h)")

print("\nKey parameters:")
print(f"  Solar PV CAPEX: ${solar_pv['capex_per_kw']}/kW")
print(f"  Solar+Storage 4h CAPEX: ${tech_rows[2]['capex_per_kw']}/kW")
print(f"  CCGT CAPEX: ${ccgt['capex_per_kw']}/kW")
print(f"  CCGT Heat Rate: {ccgt['heat_rate_mmbtu_per_mwh']} MMBtu/MWh")
print(f"  CCGT VOM: ${ccgt['vom_per_mwh']}/MWh")
print(f"  CO2 Factor: {co2_t_per_mmbtu:.5f} tCO2/MMBtu")
print(f"  CCGT CO2: {co2_t_per_mwh_ccgt:.4f} tCO2/MWh")

print("\n" + "="*70)
print("Texas Electric Power Gas Prices (2019-2023)")
print("="*70)
print("\n  Year     Price        Notes")
print("  ----     -----        -----")
print("  2019     $2.43        Normal (low gas price)")
print("  2020     $2.15        COVID-19 (low demand)")
print("  2021     $9.52        Winter Storm Uri")
print("  2022     $6.34        Russia-Ukraine, LNG exports")
print("  2023     $2.58        Return to normal")

print("\nReady for Module 2.")

"""MODULE 2.1: PV Revenue Calculation (2019-2023)"""

"""
MODULE 2.1: PV Revenue Calculation (2019-2023)
Calculates PV revenue for all zone-year combinations

Input:
  - pv_output_{ZONE}_{YEAR}.parquet (from Module 1.3)
  - ercot_dam_{ZONE}_{YEAR}.parquet (from Module 1.1)

Output:
  - pv_revenue_results.csv (summary table)
  - pv_revenue_{ZONE}_{YEAR}.parquet (detailed hourly data)
"""

import pandas as pd
import numpy as np
from pathlib import Path
import warnings
warnings.filterwarnings('ignore')

# Configuration
YEARS = [2019, 2020, 2021, 2022, 2023]
ZONES = ['HOUSTON', 'WEST']
CAPACITY_MW = 100  # Standard 100MW PV plant

OUT_DIR = Path('out')
OUT_DIR.mkdir(exist_ok=True)

print("="*70)
print("MODULE 2.1: PV Revenue Calculation (2019-2023)")
print("="*70)

results = []

for zone in ZONES:
    print(f"\n{'='*70}")
    print(f"Processing Zone: {zone}")
    print(f"{'='*70}")

    for year in YEARS:
        print(f"\nYear {year}...")

        # Load data (updated file names to match Module 1 outputs)
        pv_file = OUT_DIR / f'pv_output_{zone}_{year}.parquet'
        price_file = OUT_DIR / f'ercot_dam_{zone}_{year}.parquet'

        if not pv_file.exists():
            print(f"  ERROR: {pv_file.name} not found, skipping...")
            continue

        if not price_file.exists():
            print(f"  ERROR: {price_file.name} not found, skipping...")
            continue

        df_pv = pd.read_parquet(pv_file)
        df_price = pd.read_parquet(price_file)

        print(f"  Loaded: {len(df_pv)} PV hours, {len(df_price)} price hours")

        # Align data by datetime
        # Handle different possible column names from Module 1
        pv_dt_col = 'datetime' if 'datetime' in df_pv.columns else 'datetime_local'
        price_dt_col = 'datetime' if 'datetime' in df_price.columns else 'datetime_local'

        # Remove timezone for matching if needed
        if hasattr(df_pv[pv_dt_col].dtype, 'tz') and df_pv[pv_dt_col].dtype.tz:
            df_pv['datetime_key'] = df_pv[pv_dt_col].dt.tz_localize(None)
        else:
            df_pv['datetime_key'] = df_pv[pv_dt_col]

        if hasattr(df_price[price_dt_col].dtype, 'tz') and df_price[price_dt_col].dtype.tz:
            df_price['datetime_key'] = df_price[price_dt_col].dt.tz_localize(None)
        else:
            df_price['datetime_key'] = df_price[price_dt_col]

        # Merge on datetime
        df_merged = pd.merge(
            df_pv[['datetime_key', 'ac_mw']],
            df_price[['datetime_key', 'price_usd_per_mwh']],
            on='datetime_key',
            how='inner'
        )

        print(f"  Merged: {len(df_merged)} hours")

        if len(df_merged) == 0:
            print(f"  ERROR: No matching hours after merge!")
            continue

        # Calculate hourly revenue
        df_merged['revenue_usd'] = df_merged['ac_mw'] * df_merged['price_usd_per_mwh']

        # Annual totals
        annual_generation_mwh = df_merged['ac_mw'].sum()
        annual_revenue_usd = df_merged['revenue_usd'].sum()

        # Price statistics
        price_avg = df_merged['price_usd_per_mwh'].mean()
        price_min = df_merged['price_usd_per_mwh'].min()
        price_max = df_merged['price_usd_per_mwh'].max()

        # Generation statistics
        generation_hours = (df_merged['ac_mw'] > 0).sum()
        peak_output_mw = df_merged['ac_mw'].max()

        # Capacity factor
        hours_in_year = len(df_merged)
        capacity_factor = (annual_generation_mwh / (CAPACITY_MW * hours_in_year)) * 100

        # Value factor (ratio of average revenue price to average market price)
        avg_revenue_price = annual_revenue_usd / annual_generation_mwh if annual_generation_mwh > 0 else 0
        value_factor = (avg_revenue_price / price_avg * 100) if price_avg > 0 else 0

        print(f"  Generation: {annual_generation_mwh:,.0f} MWh")
        print(f"  Revenue: ${annual_revenue_usd:,.0f}")
        print(f"  Avg price captured: ${avg_revenue_price:.2f}/MWh")
        print(f"  Market avg price: ${price_avg:.2f}/MWh")
        print(f"  Value factor: {value_factor:.1f}%")
        print(f"  Capacity factor: {capacity_factor:.1f}%")

        # Special check for Uri (2021 only)
        if year == 2021:
            uri_mask = (df_merged['datetime_key'] >= '2021-02-12') & (df_merged['datetime_key'] <= '2021-02-20')
            uri_revenue = df_merged.loc[uri_mask, 'revenue_usd'].sum()
            uri_pct = (uri_revenue / annual_revenue_usd * 100) if annual_revenue_usd > 0 else 0
            print(f"  Uri revenue: ${uri_revenue:,.0f} ({uri_pct:.1f}% of annual)")

        # Save detailed hourly data
        output_file = OUT_DIR / f'pv_revenue_{zone}_{year}.parquet'
        df_merged.to_parquet(output_file, index=False)
        print(f"  Saved: {output_file.name}")

        # Store summary
        results.append({
            'zone': zone,
            'year': year,
            'hours': hours_in_year,
            'capacity_mw': CAPACITY_MW,
            'annual_generation_mwh': round(annual_generation_mwh, 0),
            'annual_revenue_usd': round(annual_revenue_usd, 0),
            'capacity_factor_pct': round(capacity_factor, 1),
            'avg_price_captured_usd_per_mwh': round(avg_revenue_price, 2),
            'market_avg_price_usd_per_mwh': round(price_avg, 2),
            'value_factor_pct': round(value_factor, 1),
            'peak_output_mw': round(peak_output_mw, 1),
            'generation_hours': generation_hours
        })

# Create summary table
print("\n" + "="*70)
print("SUMMARY: PV Revenue by Zone and Year")
print("="*70)

df_summary = pd.DataFrame(results)

if not df_summary.empty:
    print("\n" + df_summary.to_string(index=False))

    df_summary.to_csv(OUT_DIR / 'pv_revenue_results.csv', index=False)
    print(f"\nSummary saved: pv_revenue_results.csv")

    # Year-over-year comparison
    print("\n" + "="*70)
    print("Year-over-Year Analysis")
    print("="*70)

    for zone in ZONES:
        zone_data = df_summary[df_summary['zone'] == zone].copy()
        if len(zone_data) > 0:
            print(f"\n{zone}:")
            print(f"  Highest revenue: {zone_data.loc[zone_data['annual_revenue_usd'].idxmax(), 'year']} "
                  f"(${zone_data['annual_revenue_usd'].max():,.0f})")
            print(f"  Lowest revenue: {zone_data.loc[zone_data['annual_revenue_usd'].idxmin(), 'year']} "
                  f"(${zone_data['annual_revenue_usd'].min():,.0f})")
            print(f"  Average revenue (2019-2023): ${zone_data['annual_revenue_usd'].mean():,.0f}")

            cv = (zone_data['annual_revenue_usd'].std() / zone_data['annual_revenue_usd'].mean() * 100)
            print(f"  Revenue variability (CV): {cv:.1f}%")

    # Value factor analysis
    print("\n" + "="*70)
    print("Value Factor Analysis")
    print("="*70)
    print("\nValue factor = (Avg price captured / Market avg price) × 100%")
    print("- Below 100%: PV captures below-average prices (typical due to solar correlation)")
    print("- Above 100%: PV captures above-average prices (unusual)")

    pivot_vf = df_summary.pivot(index='year', columns='zone', values='value_factor_pct')
    print("\n" + pivot_vf.to_string())

    print("\nMODULE 2.1 COMPLETE!")
    print(f"Processed {len(results)} zone-year combinations")

else:
    print("\nNo results generated. Please check input files.")

# Display sample
print("\n" + "="*70)
print("Sample: Hourly PV Revenue (HOUSTON 2021, first 24 hours)")
print("="*70)

sample_file = OUT_DIR / 'pv_revenue_HOUSTON_2021.parquet'
if sample_file.exists():
    df_sample = pd.read_parquet(sample_file)
    print("\n" + df_sample.head(24).to_string(index=False))

    peak_rev_idx = df_sample['revenue_usd'].idxmax()
    print(f"\nPeak revenue hour:")
    print(df_sample.iloc[peak_rev_idx:peak_rev_idx+1].to_string(index=False))

"""MODULE 2.2: Storage Arbitrage Calculation (2019-2023)"""

"""
MODULE 2.2: Storage Arbitrage Calculation (2019-2023)
Calculates arbitrage revenue for Solar+Storage systems

Input:
  - pv_output_{ZONE}_{YEAR}.parquet (from Module 1.3)
  - ercot_dam_{ZONE}_{YEAR}.parquet (from Module 1.1)

Output:
  - storage_revenue_results.csv (summary table)
  - storage_revenue_{ZONE}_{YEAR}_{DURATION}h.parquet (detailed)
"""

import pandas as pd
import numpy as np
from pathlib import Path
import warnings
warnings.filterwarnings('ignore')

# Configuration
YEARS = [2019, 2020, 2021, 2022, 2023]
ZONES = ['HOUSTON', 'WEST']
DURATIONS = [2, 4]  # Battery duration in hours

BATTERY_POWER_MW = 100  # Battery power capacity
RT_EFFICIENCY = 0.85    # Roundtrip efficiency (charge and discharge)

OUT_DIR = Path('out')
OUT_DIR.mkdir(exist_ok=True)

print("="*70)
print("MODULE 2.2: Storage Arbitrage Calculation (2019-2023)")
print("="*70)


def daily_arbitrage_strategy(df_day, energy_capacity_mwh, power_capacity_mw, rt_eff):
    """
    Intra-day arbitrage strategy: charge low, discharge high
    """
    df = df_day.copy()

    # Initialization
    df['battery_charge_mw'] = 0.0
    df['battery_discharge_mw'] = 0.0
    df['soc_mwh'] = 0.0
    df['arbitrage_revenue_usd'] = 0.0

    if len(df) < 2:
        return df

    # Hours needed to fully charge/discharge
    hours_to_full = int(np.ceil(energy_capacity_mwh / power_capacity_mw))

    n_charge_hours = min(hours_to_full, len(df) // 2)
    n_discharge_hours = min(hours_to_full, len(df) // 2)

    df_sorted = df.copy()
    df_sorted['hour_idx'] = range(len(df_sorted))

    # Find N lowest price hours for charging
    df_lowest = df_sorted.nsmallest(n_charge_hours, 'price_usd_per_mwh')
    charge_hours = df_lowest.index.tolist()

    # Find N highest price hours for discharging (excluding charge hours)
    df_available = df_sorted[~df_sorted.index.isin(charge_hours)]
    if len(df_available) >= n_discharge_hours:
        df_highest = df_available.nlargest(n_discharge_hours, 'price_usd_per_mwh')
        discharge_hours = df_highest.index.tolist()
    else:
        discharge_hours = []

    # Simulate charging
    soc = 0.0
    total_charged = 0.0

    for idx in charge_hours:
        can_charge = min(power_capacity_mw, energy_capacity_mwh - soc)
        if can_charge > 0:
            df.at[idx, 'battery_charge_mw'] = can_charge
            df.at[idx, 'arbitrage_revenue_usd'] = -can_charge * df.at[idx, 'price_usd_per_mwh']
            soc += can_charge
            total_charged += can_charge

    # Energy available for discharge
    available_discharge = total_charged * rt_eff
    remaining_discharge = available_discharge

    # Simulate discharging
    for idx in discharge_hours:
        can_discharge = min(power_capacity_mw, remaining_discharge)
        if can_discharge > 0:
            df.at[idx, 'battery_discharge_mw'] = can_discharge
            df.at[idx, 'arbitrage_revenue_usd'] += can_discharge * df.at[idx, 'price_usd_per_mwh']
            remaining_discharge -= can_discharge

    return df


def calculate_storage_arbitrage(zone, year, duration_h):
    """
    Calculates storage arbitrage for a single zone-year-duration
    """
    print(f"\n  {zone} {year} ({duration_h}h battery)...")

    # Load data (updated file names)
    pv_file = OUT_DIR / f'pv_output_{zone}_{year}.parquet'
    price_file = OUT_DIR / f'ercot_dam_{zone}_{year}.parquet'

    if not pv_file.exists() or not price_file.exists():
        print(f"    ERROR: Data files not found")
        return None

    df_pv = pd.read_parquet(pv_file)
    df_price = pd.read_parquet(price_file)

    # Handle different possible column names
    pv_dt_col = 'datetime' if 'datetime' in df_pv.columns else 'datetime_local'
    price_dt_col = 'datetime' if 'datetime' in df_price.columns else 'datetime_local'

    # Remove timezone for matching
    if hasattr(df_pv[pv_dt_col].dtype, 'tz') and df_pv[pv_dt_col].dtype.tz:
        df_pv['datetime_key'] = df_pv[pv_dt_col].dt.tz_localize(None)
    else:
        df_pv['datetime_key'] = df_pv[pv_dt_col]

    if hasattr(df_price[price_dt_col].dtype, 'tz') and df_price[price_dt_col].dtype.tz:
        df_price['datetime_key'] = df_price[price_dt_col].dt.tz_localize(None)
    else:
        df_price['datetime_key'] = df_price[price_dt_col]

    df_merged = pd.merge(
        df_pv[['datetime_key', 'ac_mw']],
        df_price[['datetime_key', 'price_usd_per_mwh']],
        on='datetime_key',
        how='inner'
    )

    # Handle leap year
    expected_hours = 8784 if (year % 4 == 0 and (year % 100 != 0 or year % 400 == 0)) else 8760
    if len(df_merged) != expected_hours:
        print(f"    Warning: Merged {len(df_merged)} hours, expected {expected_hours}")

    # Calculate PV revenue
    df_merged['pv_revenue_usd'] = df_merged['ac_mw'] * df_merged['price_usd_per_mwh']

    # Battery parameters
    energy_capacity_mwh = BATTERY_POWER_MW * duration_h

    # Calculate daily arbitrage
    df_merged['date'] = pd.to_datetime(df_merged['datetime_key']).dt.date

    results = []
    for date, df_day in df_merged.groupby('date'):
        df_day_arb = daily_arbitrage_strategy(
            df_day,
            energy_capacity_mwh,
            BATTERY_POWER_MW,
            RT_EFFICIENCY
        )
        results.append(df_day_arb)

    df_final = pd.concat(results, ignore_index=True)

    # Total revenue = PV + Storage arbitrage
    df_final['total_revenue_usd'] = df_final['pv_revenue_usd'] + df_final['arbitrage_revenue_usd']

    # Annual totals
    annual_pv_revenue = df_final['pv_revenue_usd'].sum()
    annual_storage_revenue = df_final['arbitrage_revenue_usd'].sum()
    annual_total_revenue = df_final['total_revenue_usd'].sum()

    # Storage statistics
    total_charge_mwh = df_final['battery_charge_mw'].sum()
    total_discharge_mwh = df_final['battery_discharge_mw'].sum()
    charge_cycles = total_discharge_mwh / energy_capacity_mwh if energy_capacity_mwh > 0 else 0

    # Days with positive arbitrage
    daily_arb = df_final.groupby('date')['arbitrage_revenue_usd'].sum()
    days_with_arbitrage = (daily_arb > 0).sum()

    print(f"    PV revenue:      ${annual_pv_revenue:,.0f}")
    print(f"    Storage revenue: ${annual_storage_revenue:,.0f}")
    print(f"    Total revenue:   ${annual_total_revenue:,.0f}")
    print(f"    Storage gain:    {(annual_storage_revenue/annual_pv_revenue*100):.1f}% over PV-only")
    print(f"    Charge cycles:   {charge_cycles:.0f} cycles/year")
    print(f"    Active days:     {days_with_arbitrage}/365")

    # Uri analysis (2021)
    if year == 2021:
        uri_mask = (df_final['datetime_key'] >= '2021-02-12') & (df_final['datetime_key'] <= '2021-02-20')
        uri_storage_rev = df_final.loc[uri_mask, 'arbitrage_revenue_usd'].sum()
        uri_pct = (uri_storage_rev / annual_storage_revenue * 100) if annual_storage_revenue > 0 else 0
        print(f"    Uri storage rev: ${uri_storage_rev:,.0f} ({uri_pct:.1f}% of annual storage)")

    # Save
    output_file = OUT_DIR / f'storage_revenue_{zone}_{year}_{duration_h}h.parquet'
    df_final.to_parquet(output_file, index=False)
    print(f"    Saved: {output_file.name}")

    return {
        'zone': zone,
        'year': year,
        'duration_h': duration_h,
        'hours': len(df_final),
        'pv_capacity_mw': 100,
        'battery_power_mw': BATTERY_POWER_MW,
        'battery_energy_mwh': energy_capacity_mwh,
        'annual_pv_revenue_usd': round(annual_pv_revenue, 0),
        'annual_storage_revenue_usd': round(annual_storage_revenue, 0),
        'annual_total_revenue_usd': round(annual_total_revenue, 0),
        'storage_revenue_pct_of_pv': round((annual_storage_revenue/annual_pv_revenue*100), 1) if annual_pv_revenue > 0 else 0,
        'charge_cycles_per_year': round(charge_cycles, 1),
        'days_with_arbitrage': days_with_arbitrage,
        'total_charge_mwh': round(total_charge_mwh, 0),
        'total_discharge_mwh': round(total_discharge_mwh, 0),
        'roundtrip_efficiency': RT_EFFICIENCY
    }


# Main execution
results = []

for zone in ZONES:
    print(f"\n{'='*70}")
    print(f"Processing Zone: {zone}")
    print(f"{'='*70}")

    for year in YEARS:
        for duration_h in DURATIONS:
            result = calculate_storage_arbitrage(zone, year, duration_h)
            if result:
                results.append(result)

# Summary
print("\n" + "="*70)
print("SUMMARY: Storage Arbitrage Results")
print("="*70)

df_summary = pd.DataFrame(results)

if not df_summary.empty:
    print("\n" + df_summary.to_string(index=False))

    df_summary.to_csv(OUT_DIR / 'storage_revenue_results.csv', index=False)
    print(f"\nSummary saved: storage_revenue_results.csv")

    # 2h vs 4h comparison
    print("\n" + "="*70)
    print("2h vs 4h Battery Comparison")
    print("="*70)

    for zone in ZONES:
        print(f"\n{zone}:")
        zone_data = df_summary[df_summary['zone'] == zone]

        for year in YEARS:
            year_data = zone_data[zone_data['year'] == year]
            if len(year_data) == 2:
                rev_2h = year_data[year_data['duration_h'] == 2]['annual_storage_revenue_usd'].values[0]
                rev_4h = year_data[year_data['duration_h'] == 4]['annual_storage_revenue_usd'].values[0]
                improvement = ((rev_4h - rev_2h) / abs(rev_2h) * 100) if rev_2h != 0 else 0
                print(f"  {year}: 2h=${rev_2h:,.0f}, 4h=${rev_4h:,.0f} ({improvement:+.1f}%)")

    # Best/worst years
    print("\n" + "="*70)
    print("Best Years for Storage Arbitrage (4h battery)")
    print("="*70)

    for zone in ZONES:
        zone_4h = df_summary[(df_summary['zone'] == zone) & (df_summary['duration_h'] == 4)]
        if not zone_4h.empty:
            best_year = zone_4h.loc[zone_4h['annual_storage_revenue_usd'].idxmax()]
            worst_year = zone_4h.loc[zone_4h['annual_storage_revenue_usd'].idxmin()]

            print(f"\n{zone}:")
            print(f"  Best:  {int(best_year['year'])} - ${best_year['annual_storage_revenue_usd']:,.0f}")
            print(f"  Worst: {int(worst_year['year'])} - ${worst_year['annual_storage_revenue_usd']:,.0f}")
            print(f"  Avg storage gain over PV: {zone_4h['storage_revenue_pct_of_pv'].mean():.1f}%")

    print("\nMODULE 2.2 COMPLETE!")
    print(f"Processed {len(results)} zone-year-duration combinations")

else:
    print("\nNo results generated. Please check input files.")

# Sample output
print("\n" + "="*70)
print("Sample: Storage Operation (HOUSTON 2021, 4h, first 3 days)")
print("="*70)

sample_file = OUT_DIR / 'storage_revenue_HOUSTON_2021_4h.parquet'
if sample_file.exists():
    df_sample = pd.read_parquet(sample_file)
    df_sample_3days = df_sample.head(72)

    df_active = df_sample_3days[
        (df_sample_3days['battery_charge_mw'] > 0) |
        (df_sample_3days['battery_discharge_mw'] > 0)
    ]

    if not df_active.empty:
        print("\nBattery activity (charge/discharge hours only):")
        print(df_active[['datetime_key', 'price_usd_per_mwh', 'battery_charge_mw',
                         'battery_discharge_mw', 'arbitrage_revenue_usd']].to_string(index=False))

    # Best arbitrage day
    df_sample['date'] = pd.to_datetime(df_sample['datetime_key']).dt.date
    daily_arb = df_sample.groupby('date')['arbitrage_revenue_usd'].sum().sort_values(ascending=False)

    if len(daily_arb) > 0:
        best_day = daily_arb.index[0]
        print(f"\n\nBest arbitrage day: {best_day} (${daily_arb.iloc[0]:,.2f})")

        df_best_day = df_sample[df_sample['date'] == best_day]
        df_best_active = df_best_day[
            (df_best_day['battery_charge_mw'] > 0) |
            (df_best_day['battery_discharge_mw'] > 0)
        ]

        if not df_best_active.empty:
            print("\nBattery operations on best day:")
            print(df_best_active[['datetime_key', 'price_usd_per_mwh', 'battery_charge_mw',
                                   'battery_discharge_mw', 'arbitrage_revenue_usd']].to_string(index=False))

"""2.3"""

import pandas as pd
import numpy as np
from pathlib import Path
import warnings
warnings.filterwarnings('ignore')

# Configuration
YEARS = [2019, 2020, 2021, 2022, 2023]
ZONES = ['HOUSTON', 'WEST']
CAPACITY_MW = 100  # Standard 100MW CCGT plant

OUT_DIR = Path('out')
OUT_DIR.mkdir(exist_ok=True)

print("="*70)
print("MODULE 2.3: CCGT Dispatch Engine (2019-2023)")
print("="*70)

# Load parameters
print("\nLoading parameters...")
df_tech = pd.read_csv(OUT_DIR / 'tech_params.csv')
df_policy = pd.read_csv(OUT_DIR / 'policy_params.csv')

# Extract CCGT parameters
ccgt_params = df_tech[df_tech['tech'] == 'CCGT'].iloc[0]
HEAT_RATE = ccgt_params['heat_rate_mmbtu_per_mwh']  # MMBtu/MWh
VOM = ccgt_params['vom_per_mwh']  # $/MWh
CAPEX = ccgt_params['capex_per_kw']  # $/kW
FOM = ccgt_params['fom_per_kw_yr']  # $/kW-yr

print(f"  CCGT Heat Rate: {HEAT_RATE:.4f} MMBtu/MWh")
print(f"  CCGT VOM: ${VOM:.3f}/MWh")
print(f"  CCGT CAPEX: ${CAPEX}/kW")
print(f"  CCGT FOM: ${FOM}/kW-yr")

# Extract carbon parameters
CO2_FACTOR = df_policy['co2_t_per_mmbtu'].iloc[0]  # tCO2/MMBtu
CO2_PER_MWH = df_policy['co2_t_per_mwh_ccgt'].iloc[0]  # tCO2/MWh

print(f"  CO2 emission factor: {CO2_FACTOR:.5f} tCO2/MMBtu")
print(f"  CCGT CO2 emissions: {CO2_PER_MWH:.6f} tCO2/MWh")


def get_expected_hours(year):
    """Calculate expected hours for a year."""
    is_leap = (year % 4 == 0) and (year % 100 != 0 or year % 400 == 0)
    return 8784 if is_leap else 8760


def calculate_ccgt_dispatch(zone, year, carbon_price_usd_per_t=0):
    """
    Calculates CCGT dispatch for a single zone-year
    """
    scenario_name = f"historical_{year}" if carbon_price_usd_per_t == 0 else f"carbon_{int(carbon_price_usd_per_t)}"

    print(f"\n  {zone} {year} (carbon ${carbon_price_usd_per_t}/tCO2)...")

    # Load data (updated file names)
    price_file = OUT_DIR / f'ercot_dam_{zone}_{year}.parquet'
    gas_file = OUT_DIR / f'gas_prices_hourly_{year}.parquet'

    if not price_file.exists():
        print(f"    ERROR: {price_file.name} not found")
        return None
    if not gas_file.exists():
        print(f"    ERROR: {gas_file.name} not found")
        return None

    df_price = pd.read_parquet(price_file)
    df_gas = pd.read_parquet(gas_file)

    # Handle different possible column names
    price_dt_col = 'datetime' if 'datetime' in df_price.columns else 'datetime_local'
    gas_dt_col = 'datetime' if 'datetime' in df_gas.columns else 'datetime_local'

    # Remove timezone for matching
    if hasattr(df_price[price_dt_col].dtype, 'tz') and df_price[price_dt_col].dtype.tz:
        df_price['datetime_key'] = df_price[price_dt_col].dt.tz_localize(None)
    else:
        df_price['datetime_key'] = df_price[price_dt_col]

    if hasattr(df_gas[gas_dt_col].dtype, 'tz') and df_gas[gas_dt_col].dtype.tz:
        df_gas['datetime_key'] = df_gas[gas_dt_col].dt.tz_localize(None)
    else:
        df_gas['datetime_key'] = df_gas[gas_dt_col]

    df_merged = pd.merge(
        df_price[['datetime_key', 'price_usd_per_mwh']],
        df_gas[['datetime_key', 'gas_price_usd_mmbtu']],
        on='datetime_key',
        how='inner'
    )

    expected_hours = get_expected_hours(year)
    if len(df_merged) != expected_hours:
        print(f"    Warning: Merged {len(df_merged)} hours, expected {expected_hours}")

    # Calculate marginal cost components (hourly)
    df_merged['fuel_cost_usd_per_mwh'] = df_merged['gas_price_usd_mmbtu'] * HEAT_RATE
    df_merged['carbon_cost_usd_per_mwh'] = carbon_price_usd_per_t * CO2_PER_MWH
    df_merged['vom_usd_per_mwh'] = VOM

    # Total marginal cost
    df_merged['marginal_cost_usd_per_mwh'] = (
        df_merged['fuel_cost_usd_per_mwh'] +
        df_merged['carbon_cost_usd_per_mwh'] +
        df_merged['vom_usd_per_mwh']
    )

    # Dispatch decision (hourly)
    df_merged['is_economic'] = df_merged['price_usd_per_mwh'] >= df_merged['marginal_cost_usd_per_mwh']
    df_merged['generation_mw'] = df_merged['is_economic'].astype(float) * CAPACITY_MW

    # Revenue and costs (hourly)
    df_merged['revenue_usd'] = df_merged['generation_mw'] * df_merged['price_usd_per_mwh']
    df_merged['fuel_cost_usd'] = df_merged['generation_mw'] * df_merged['fuel_cost_usd_per_mwh']
    df_merged['carbon_cost_usd'] = df_merged['generation_mw'] * df_merged['carbon_cost_usd_per_mwh']
    df_merged['vom_cost_usd'] = df_merged['generation_mw'] * df_merged['vom_usd_per_mwh']

    # Gross margin (hourly)
    df_merged['gross_margin_usd'] = (
        df_merged['revenue_usd'] -
        df_merged['fuel_cost_usd'] -
        df_merged['carbon_cost_usd'] -
        df_merged['vom_cost_usd']
    )

    # Annual totals
    annual_generation_mwh = df_merged['generation_mw'].sum()
    annual_revenue_usd = df_merged['revenue_usd'].sum()
    annual_fuel_cost_usd = df_merged['fuel_cost_usd'].sum()
    annual_carbon_cost_usd = df_merged['carbon_cost_usd'].sum()
    annual_vom_cost_usd = df_merged['vom_cost_usd'].sum()
    annual_gross_margin_usd = df_merged['gross_margin_usd'].sum()

    # Operating statistics
    hours_running = df_merged['is_economic'].sum()
    hours_shutdown = len(df_merged) - hours_running
    capacity_factor = (annual_generation_mwh / (CAPACITY_MW * len(df_merged))) * 100

    # Average prices when running
    if hours_running > 0:
        avg_lmp_when_running = df_merged.loc[df_merged['is_economic'], 'price_usd_per_mwh'].mean()
        avg_marginal_cost_when_running = df_merged.loc[df_merged['is_economic'], 'marginal_cost_usd_per_mwh'].mean()
        avg_gas_price_when_running = df_merged.loc[df_merged['is_economic'], 'gas_price_usd_mmbtu'].mean()
    else:
        avg_lmp_when_running = 0
        avg_marginal_cost_when_running = 0
        avg_gas_price_when_running = 0

    # Emissions
    annual_co2_tons = annual_generation_mwh * CO2_PER_MWH

    print(f"    Generation: {annual_generation_mwh:,.0f} MWh")
    print(f"    Capacity factor: {capacity_factor:.1f}%")
    print(f"    Hours running: {hours_running}/{len(df_merged)} ({hours_running/len(df_merged)*100:.1f}%)")
    print(f"    Revenue: ${annual_revenue_usd:,.0f}")
    print(f"    Fuel cost: ${annual_fuel_cost_usd:,.0f}")
    print(f"    Carbon cost: ${annual_carbon_cost_usd:,.0f}")
    print(f"    VOM cost: ${annual_vom_cost_usd:,.0f}")
    print(f"    Gross margin: ${annual_gross_margin_usd:,.0f}")
    print(f"    Avg LMP (when running): ${avg_lmp_when_running:.2f}/MWh")
    print(f"    Avg marginal cost: ${avg_marginal_cost_when_running:.2f}/MWh")

    # Special check for Uri (2021)
    if year == 2021:
        uri_mask = (df_merged['datetime_key'] >= '2021-02-12') & (df_merged['datetime_key'] <= '2021-02-20')
        uri_generation = df_merged.loc[uri_mask, 'generation_mw'].sum()
        uri_revenue = df_merged.loc[uri_mask, 'revenue_usd'].sum()
        uri_fuel_cost = df_merged.loc[uri_mask, 'fuel_cost_usd'].sum()
        uri_margin = df_merged.loc[uri_mask, 'gross_margin_usd'].sum()
        uri_hours_running = df_merged.loc[uri_mask, 'is_economic'].sum()
        uri_pct = (uri_margin / annual_gross_margin_usd * 100) if annual_gross_margin_usd > 0 else 0

        print(f"    Uri period (Feb 12-20):")
        print(f"      Hours running: {uri_hours_running}/216")
        print(f"      Revenue: ${uri_revenue:,.0f}")
        print(f"      Fuel cost: ${uri_fuel_cost:,.0f}")
        print(f"      Margin: ${uri_margin:,.0f} ({uri_pct:.1f}% of annual)")

    # Save detailed data
    output_file = OUT_DIR / f'ccgt_dispatch_{zone}_{year}_{scenario_name}.parquet'
    df_merged.to_parquet(output_file, index=False)
    print(f"    Saved: {output_file.name}")

    return {
        'zone': zone,
        'year': year,
        'scenario': scenario_name,
        'carbon_price_usd_per_t': carbon_price_usd_per_t,
        'hours': len(df_merged),
        'capacity_mw': CAPACITY_MW,
        'heat_rate_mmbtu_per_mwh': HEAT_RATE,
        'vom_usd_per_mwh': VOM,
        'annual_generation_mwh': round(annual_generation_mwh, 0),
        'capacity_factor_pct': round(capacity_factor, 1),
        'hours_running': hours_running,
        'hours_shutdown': hours_shutdown,
        'annual_revenue_usd': round(annual_revenue_usd, 0),
        'annual_fuel_cost_usd': round(annual_fuel_cost_usd, 0),
        'annual_carbon_cost_usd': round(annual_carbon_cost_usd, 0),
        'annual_vom_cost_usd': round(annual_vom_cost_usd, 0),
        'annual_gross_margin_usd': round(annual_gross_margin_usd, 0),
        'annual_co2_tons': round(annual_co2_tons, 0),
        'avg_lmp_when_running': round(avg_lmp_when_running, 2),
        'avg_marginal_cost': round(avg_marginal_cost_when_running, 2),
        'avg_gas_price_when_running': round(avg_gas_price_when_running, 2)
    }


# Main execution
results = []

# Historical scenarios (no carbon price)
print("\n" + "="*70)
print("HISTORICAL SCENARIOS (Carbon Price = $0/tCO2)")
print("="*70)

for zone in ZONES:
    print(f"\n{'='*70}")
    print(f"Processing Zone: {zone}")
    print(f"{'='*70}")

    for year in YEARS:
        result = calculate_ccgt_dispatch(zone, year, carbon_price_usd_per_t=0)
        if result:
            results.append(result)

# Carbon sensitivity scenarios (2021 only)
print("\n" + "="*70)
print("CARBON SENSITIVITY SCENARIOS (2021 only)")
print("="*70)

carbon_prices = [50, 100]  # $/tCO2

for carbon_price in carbon_prices:
    print(f"\n{'='*70}")
    print(f"Carbon Price: ${carbon_price}/tCO2")
    print(f"{'='*70}")

    for zone in ZONES:
        result = calculate_ccgt_dispatch(zone, 2021, carbon_price_usd_per_t=carbon_price)
        if result:
            results.append(result)

# Summary
print("\n" + "="*70)
print("SUMMARY: CCGT Dispatch Results")
print("="*70)

df_summary = pd.DataFrame(results)

if not df_summary.empty:
    print("\n" + df_summary.to_string(index=False))

    df_summary.to_csv(OUT_DIR / 'ccgt_dispatch_results.csv', index=False)
    print(f"\nSummary saved: ccgt_dispatch_results.csv")

    # Historical analysis
    print("\n" + "="*70)
    print("Historical Performance Analysis (2019-2023)")
    print("="*70)

    df_hist = df_summary[df_summary['carbon_price_usd_per_t'] == 0]

    for zone in ZONES:
        zone_data = df_hist[df_hist['zone'] == zone]
        if not zone_data.empty:
            print(f"\n{zone}:")
            print(f"  Best year: {int(zone_data.loc[zone_data['annual_gross_margin_usd'].idxmax(), 'year'])} "
                  f"(${zone_data['annual_gross_margin_usd'].max():,.0f})")
            print(f"  Worst year: {int(zone_data.loc[zone_data['annual_gross_margin_usd'].idxmin(), 'year'])} "
                  f"(${zone_data['annual_gross_margin_usd'].min():,.0f})")
            print(f"  Average margin: ${zone_data['annual_gross_margin_usd'].mean():,.0f}")
            print(f"  Average CF: {zone_data['capacity_factor_pct'].mean():.1f}%")

    # Gas price impact
    print("\n" + "="*70)
    print("Gas Price Impact on CCGT Economics")
    print("="*70)

    for zone in ZONES:
        zone_data = df_hist[df_hist['zone'] == zone]
        if not zone_data.empty:
            print(f"\n{zone}:")
            for _, row in zone_data.iterrows():
                print(f"  {int(row['year'])}: Gas ${row['avg_gas_price_when_running']:.2f}/MMBtu, "
                      f"CF {row['capacity_factor_pct']:.1f}%, "
                      f"Margin ${row['annual_gross_margin_usd']:,.0f}")

    # Carbon sensitivity (2021)
    print("\n" + "="*70)
    print("Carbon Price Sensitivity (2021)")
    print("="*70)

    df_2021 = df_summary[df_summary['year'] == 2021]

    for zone in ZONES:
        zone_2021 = df_2021[df_2021['zone'] == zone]
        if not zone_2021.empty:
            print(f"\n{zone}:")
            for _, row in zone_2021.iterrows():
                print(f"  Carbon ${int(row['carbon_price_usd_per_t'])}/tCO2: "
                      f"CF {row['capacity_factor_pct']:.1f}%, "
                      f"Margin ${row['annual_gross_margin_usd']:,.0f}, "
                      f"Carbon cost ${row['annual_carbon_cost_usd']:,.0f}")

    print("\nMODULE 2.3 COMPLETE!")
    print(f"Processed {len(results)} scenarios")

else:
    print("\nNo results generated. Please check input files.")

# Sample output
print("\n" + "="*70)
print("Sample: CCGT Operation (HOUSTON 2021, first 48 hours)")
print("="*70)

sample_file = OUT_DIR / 'ccgt_dispatch_HOUSTON_2021_historical_2021.parquet'
if sample_file.exists():
    df_sample = pd.read_parquet(sample_file)
    df_sample_48h = df_sample.head(48)

    print("\nFirst 48 hours:")
    print(df_sample_48h[['datetime_key', 'price_usd_per_mwh', 'gas_price_usd_mmbtu',
                          'marginal_cost_usd_per_mwh', 'is_economic', 'generation_mw',
                          'gross_margin_usd']].to_string(index=False))

    # Uri period sample
    uri_mask = (df_sample['datetime_key'] >= '2021-02-15') & (df_sample['datetime_key'] <= '2021-02-16')
    df_uri_sample = df_sample[uri_mask]

    if not df_uri_sample.empty:
        print("\n" + "="*70)
        print("Uri Period Sample (Feb 15-16, 2021)")
        print("="*70)
        print("\n" + df_uri_sample[['datetime_key', 'price_usd_per_mwh', 'gas_price_usd_mmbtu',
                                     'marginal_cost_usd_per_mwh', 'is_economic', 'generation_mw',
                                     'gross_margin_usd']].to_string(index=False))

"""
MODULE 3.1: LCOE Calculation (Levelized Cost of Energy)
Calculates the Levelized Cost of Energy for all technologies

Input:
  - pv_revenue_results.csv (from Module 2.1)
  - storage_revenue_results.csv (from Module 2.2)
  - ccgt_dispatch_results.csv (from Module 2.3)
  - tech_params.csv (technology parameters)

Output:
  - lcoe_results.csv (LCOE for all technologies)
"""

import pandas as pd
import numpy as np
from pathlib import Path
import warnings
warnings.filterwarnings('ignore')

OUT_DIR = Path('out')
OUT_DIR.mkdir(exist_ok=True)

print("="*70)
print("MODULE 3.1: LCOE Calculation")
print("="*70)

# Load parameters
print("\nLoading parameters...")
df_tech = pd.read_csv(OUT_DIR / 'tech_params.csv')

print("\nTechnology parameters:")
print(df_tech.to_string(index=False))

# Load revenue results
print("\n" + "="*70)
print("Loading revenue results...")
print("="*70)

df_pv = pd.read_csv(OUT_DIR / 'pv_revenue_results.csv')
df_storage = pd.read_csv(OUT_DIR / 'storage_revenue_results.csv')
df_ccgt = pd.read_csv(OUT_DIR / 'ccgt_dispatch_results.csv')

print(f"  PV results: {len(df_pv)} rows")
print(f"  Storage results: {len(df_storage)} rows")
print(f"  CCGT results: {len(df_ccgt)} rows")

# PV generation by zone (from Module 2.1 - fixed for TMY data)
PV_GENERATION = {
    'HOUSTON': 171301,  # MWh/year
    'WEST': 211792      # MWh/year
}

# Leap year adjustment
PV_GENERATION_LEAP = {
    'HOUSTON': 172021,  # MWh/year (2020)
    'WEST': 212367      # MWh/year (2020)
}


def calculate_annualized_capex(capex_per_kw, capacity_mw, wacc, lifetime_years):
    """
    Calculates annualized CAPEX (Capital Recovery Factor method)
    """
    total_capex = capex_per_kw * capacity_mw * 1000

    # Capital Recovery Factor
    crf = (wacc * (1 + wacc)**lifetime_years) / ((1 + wacc)**lifetime_years - 1)

    annualized_capex = total_capex * crf

    return annualized_capex


def calculate_lcoe_pv(row, tech_params):
    """Calculates LCOE for PV-only"""
    tech = 'Solar_PV'
    params = tech_params[tech_params['tech'] == tech].iloc[0]

    capacity_mw = row['capacity_mw']
    annual_generation_mwh = row['annual_generation_mwh']

    # CAPEX
    annualized_capex = calculate_annualized_capex(
        params['capex_per_kw'],
        capacity_mw,
        params['wacc'],
        params['lifetime_years']
    )

    # FOM
    annual_fom = params['fom_per_kw_yr'] * capacity_mw * 1000

    # VOM - PV has no VOM
    annual_vom = 0

    # Total annual cost
    total_annual_cost = annualized_capex + annual_fom + annual_vom

    # LCOE
    lcoe = total_annual_cost / annual_generation_mwh if annual_generation_mwh > 0 else np.inf

    return {
        'technology': 'Solar_PV',
        'zone': row['zone'],
        'year': row['year'],
        'capacity_mw': capacity_mw,
        'annual_generation_mwh': annual_generation_mwh,
        'capex_per_kw': params['capex_per_kw'],
        'annualized_capex_usd': round(annualized_capex, 0),
        'annual_fom_usd': round(annual_fom, 0),
        'annual_vom_usd': round(annual_vom, 0),
        'total_annual_cost_usd': round(total_annual_cost, 0),
        'lcoe_usd_per_mwh': round(lcoe, 2),
        'wacc': params['wacc'],
        'lifetime_years': params['lifetime_years']
    }


def calculate_lcoe_storage(row, tech_params):
    """Calculates LCOE for Solar+Storage"""
    duration_h = row['duration_h']
    tech = f'Solar_PV_Storage_{duration_h}h'

    params = tech_params[tech_params['tech'] == tech].iloc[0]

    pv_capacity_mw = row['pv_capacity_mw']

    # Get PV generation based on zone and year
    year = row['year']
    zone = row['zone']
    if year == 2020:  # Leap year
        annual_generation_mwh = PV_GENERATION_LEAP[zone]
    else:
        annual_generation_mwh = PV_GENERATION[zone]

    # CAPEX
    annualized_capex = calculate_annualized_capex(
        params['capex_per_kw'],
        pv_capacity_mw,
        params['wacc'],
        params['lifetime_years']
    )

    # FOM
    annual_fom = params['fom_per_kw_yr'] * pv_capacity_mw * 1000

    # VOM
    annual_vom = 0

    # Total annual cost
    total_annual_cost = annualized_capex + annual_fom + annual_vom

    # LCOE
    lcoe = total_annual_cost / annual_generation_mwh if annual_generation_mwh > 0 else np.inf

    return {
        'technology': tech,
        'zone': row['zone'],
        'year': row['year'],
        'capacity_mw': pv_capacity_mw,
        'annual_generation_mwh': annual_generation_mwh,
        'capex_per_kw': params['capex_per_kw'],
        'annualized_capex_usd': round(annualized_capex, 0),
        'annual_fom_usd': round(annual_fom, 0),
        'annual_vom_usd': round(annual_vom, 0),
        'total_annual_cost_usd': round(total_annual_cost, 0),
        'lcoe_usd_per_mwh': round(lcoe, 2),
        'wacc': params['wacc'],
        'lifetime_years': params['lifetime_years']
    }


def calculate_lcoe_ccgt(row, tech_params):
    """Calculates LCOE for CCGT"""
    tech = 'CCGT'
    params = tech_params[tech_params['tech'] == tech].iloc[0]

    capacity_mw = row['capacity_mw']
    annual_generation_mwh = row['annual_generation_mwh']

    # CAPEX
    annualized_capex = calculate_annualized_capex(
        params['capex_per_kw'],
        capacity_mw,
        params['wacc'],
        params['lifetime_years']
    )

    # FOM
    annual_fom = params['fom_per_kw_yr'] * capacity_mw * 1000

    # VOM - from dispatch results
    annual_vom = row['annual_vom_cost_usd']

    # Fuel cost - from dispatch results
    annual_fuel = row['annual_fuel_cost_usd']

    # Carbon cost - from dispatch results
    annual_carbon = row['annual_carbon_cost_usd']

    # Total annual cost (including fuel and carbon)
    total_annual_cost = annualized_capex + annual_fom + annual_vom + annual_fuel + annual_carbon

    # LCOE
    lcoe = total_annual_cost / annual_generation_mwh if annual_generation_mwh > 0 else np.inf

    return {
        'technology': 'CCGT',
        'zone': row['zone'],
        'year': row['year'],
        'scenario': row['scenario'],
        'carbon_price': row['carbon_price_usd_per_t'],
        'capacity_mw': capacity_mw,
        'annual_generation_mwh': annual_generation_mwh,
        'capacity_factor_pct': row['capacity_factor_pct'],
        'capex_per_kw': params['capex_per_kw'],
        'annualized_capex_usd': round(annualized_capex, 0),
        'annual_fom_usd': round(annual_fom, 0),
        'annual_vom_usd': round(annual_vom, 0),
        'annual_fuel_cost_usd': round(annual_fuel, 0),
        'annual_carbon_cost_usd': round(annual_carbon, 0),
        'total_annual_cost_usd': round(total_annual_cost, 0),
        'lcoe_usd_per_mwh': round(lcoe, 2),
        'wacc': params['wacc'],
        'lifetime_years': params['lifetime_years']
    }


# Calculate LCOE for all technologies
print("\n" + "="*70)
print("Calculating LCOE...")
print("="*70)

results = []

# PV-only
print("\n1. Solar PV:")
for _, row in df_pv.iterrows():
    lcoe_result = calculate_lcoe_pv(row, df_tech)
    results.append(lcoe_result)
    print(f"  {row['zone']} {row['year']}: LCOE = ${lcoe_result['lcoe_usd_per_mwh']:.2f}/MWh")

# Solar+Storage
print("\n2. Solar+Storage (2h and 4h):")
for _, row in df_storage.iterrows():
    lcoe_result = calculate_lcoe_storage(row, df_tech)
    if lcoe_result:
        results.append(lcoe_result)
        print(f"  {row['zone']} {row['year']} {row['duration_h']}h: "
              f"LCOE = ${lcoe_result['lcoe_usd_per_mwh']:.2f}/MWh")

# CCGT (historical scenarios only)
print("\n3. CCGT (historical scenarios):")
df_ccgt_hist = df_ccgt[df_ccgt['carbon_price_usd_per_t'] == 0]
for _, row in df_ccgt_hist.iterrows():
    lcoe_result = calculate_lcoe_ccgt(row, df_tech)
    results.append(lcoe_result)
    print(f"  {row['zone']} {row['year']}: "
          f"LCOE = ${lcoe_result['lcoe_usd_per_mwh']:.2f}/MWh "
          f"(CF={row['capacity_factor_pct']:.1f}%)")

# Create summary DataFrame
df_lcoe = pd.DataFrame(results)

# Save detailed results
df_lcoe.to_csv(OUT_DIR / 'lcoe_results.csv', index=False)
print(f"\nDetailed results saved: lcoe_results.csv")

# Summary
print("\n" + "="*70)
print("LCOE Summary by Technology")
print("="*70)

lcoe_summary = df_lcoe.groupby('technology').agg({
    'lcoe_usd_per_mwh': ['mean', 'min', 'max', 'std']
}).round(2)

print("\n" + lcoe_summary.to_string())

# Year-by-year comparison
print("\n" + "="*70)
print("LCOE by Year (Average across zones)")
print("="*70)

df_lcoe_hist = df_lcoe[~df_lcoe['technology'].str.contains('carbon', na=False)]

for year in sorted(df_lcoe_hist['year'].unique()):
    year_data = df_lcoe_hist[df_lcoe_hist['year'] == year]
    print(f"\n{int(year)}:")

    for tech in ['Solar_PV', 'Solar_PV_Storage_2h', 'Solar_PV_Storage_4h', 'CCGT']:
        tech_data = year_data[year_data['technology'] == tech]
        if not tech_data.empty:
            avg_lcoe = tech_data['lcoe_usd_per_mwh'].mean()
            print(f"  {tech:25s}: ${avg_lcoe:6.2f}/MWh")

# Zone comparison
print("\n" + "="*70)
print("LCOE by Zone (2019-2023 average)")
print("="*70)

for zone in ['HOUSTON', 'WEST']:
    zone_data = df_lcoe_hist[df_lcoe_hist['zone'] == zone]
    print(f"\n{zone}:")

    for tech in ['Solar_PV', 'Solar_PV_Storage_2h', 'Solar_PV_Storage_4h', 'CCGT']:
        tech_data = zone_data[zone_data['technology'] == tech]
        if not tech_data.empty:
            avg_lcoe = tech_data['lcoe_usd_per_mwh'].mean()
            min_lcoe = tech_data['lcoe_usd_per_mwh'].min()
            max_lcoe = tech_data['lcoe_usd_per_mwh'].max()
            print(f"  {tech:25s}: ${avg_lcoe:6.2f}/MWh (range: ${min_lcoe:.2f}-${max_lcoe:.2f})")

# Key insights
print("\n" + "="*70)
print("Key Insights")
print("="*70)

print("\nLowest LCOE by year:")
for year in sorted(df_lcoe_hist['year'].unique()):
    year_data = df_lcoe_hist[df_lcoe_hist['year'] == year]
    lowest = year_data.loc[year_data['lcoe_usd_per_mwh'].idxmin()]
    print(f"  {int(year)}: {lowest['technology']} (${lowest['lcoe_usd_per_mwh']:.2f}/MWh) - {lowest['zone']}")

# CCGT LCOE variability
print("\n" + "="*70)
print("CCGT LCOE Analysis (capacity factor impact)")
print("="*70)

ccgt_data = df_lcoe_hist[df_lcoe_hist['technology'] == 'CCGT'].copy()
ccgt_data = ccgt_data.sort_values('capacity_factor_pct')

print("\nCCGT LCOE vs Capacity Factor:")
for _, row in ccgt_data.iterrows():
    fuel_per_mwh = row['annual_fuel_cost_usd'] / row['annual_generation_mwh'] if row['annual_generation_mwh'] > 0 else 0
    print(f"  {row['zone']} {int(row['year'])}: "
          f"CF={row['capacity_factor_pct']:5.1f}%, "
          f"LCOE=${row['lcoe_usd_per_mwh']:6.2f}/MWh, "
          f"Fuel=${fuel_per_mwh:5.2f}/MWh")

print("\nMODULE 3.1 COMPLETE.")
print(f"Calculated LCOE for {len(results)} technology-zone-year combinations")

"""3.2"""

import pandas as pd
import numpy as np
from pathlib import Path
import warnings
warnings.filterwarnings('ignore')

OUT_DIR = Path('out')
OUT_DIR.mkdir(exist_ok=True)

print("="*70)
print("MODULE 3.2: NPV and Financial Metrics Calculation")
print("="*70)

# Load parameters
print("\nLoading parameters...")
df_tech = pd.read_csv(OUT_DIR / 'tech_params.csv')

# Load revenue results
print("Loading revenue results...")
df_pv = pd.read_csv(OUT_DIR / 'pv_revenue_results.csv')
df_storage = pd.read_csv(OUT_DIR / 'storage_revenue_results.csv')
df_ccgt = pd.read_csv(OUT_DIR / 'ccgt_dispatch_results.csv')

print(f"  PV results: {len(df_pv)} rows")
print(f"  Storage results: {len(df_storage)} rows")
print(f"  CCGT results: {len(df_ccgt)} rows")


def calculate_npv(annual_cash_flows, capex, wacc, lifetime_years):
    """Calculates NPV"""
    pv_cash_flows = sum([annual_cash_flows / (1 + wacc)**t
                         for t in range(1, lifetime_years + 1)])
    npv = pv_cash_flows - capex
    return npv


def calculate_payback_period(annual_cash_flows, capex):
    """Calculates simple payback period"""
    if annual_cash_flows <= 0:
        return np.inf
    return capex / annual_cash_flows


def calculate_profitability_index(annual_cash_flows, capex, wacc, lifetime_years):
    """Calculates profitability index"""
    pv_cash_flows = sum([annual_cash_flows / (1 + wacc)**t
                         for t in range(1, lifetime_years + 1)])
    return pv_cash_flows / capex if capex > 0 else np.inf


def calculate_npv_pv(row, tech_params):
    """Calculates NPV for PV-only"""
    tech = 'Solar_PV'
    params = tech_params[tech_params['tech'] == tech].iloc[0]

    capacity_mw = row['capacity_mw']
    annual_revenue = row['annual_revenue_usd']

    # CAPEX
    total_capex = params['capex_per_kw'] * capacity_mw * 1000

    # Annual costs
    annual_fom = params['fom_per_kw_yr'] * capacity_mw * 1000
    annual_vom = 0

    # Annual cash flow
    annual_cash_flow = annual_revenue - annual_fom - annual_vom

    # NPV
    npv = calculate_npv(annual_cash_flow, total_capex, params['wacc'], params['lifetime_years'])
    payback = calculate_payback_period(annual_cash_flow, total_capex)
    pi = calculate_profitability_index(annual_cash_flow, total_capex, params['wacc'], params['lifetime_years'])

    return {
        'technology': 'Solar_PV',
        'zone': row['zone'],
        'year': row['year'],
        'capacity_mw': capacity_mw,
        'capex_total_usd': round(total_capex, 0),
        'annual_revenue_usd': round(annual_revenue, 0),
        'annual_fom_usd': round(annual_fom, 0),
        'annual_vom_usd': round(annual_vom, 0),
        'annual_cash_flow_usd': round(annual_cash_flow, 0),
        'npv_usd': round(npv, 0),
        'payback_years': round(payback, 1),
        'profitability_index': round(pi, 2),
        'wacc': params['wacc'],
        'lifetime_years': params['lifetime_years']
    }


def calculate_npv_storage(row, tech_params):
    """Calculates NPV for Solar+Storage"""
    duration_h = row['duration_h']
    tech = f'Solar_PV_Storage_{duration_h}h'

    params = tech_params[tech_params['tech'] == tech].iloc[0]

    capacity_mw = row['pv_capacity_mw']
    annual_revenue = row['annual_total_revenue_usd']

    # CAPEX
    total_capex = params['capex_per_kw'] * capacity_mw * 1000

    # Annual costs
    annual_fom = params['fom_per_kw_yr'] * capacity_mw * 1000
    annual_vom = 0

    # Annual cash flow
    annual_cash_flow = annual_revenue - annual_fom - annual_vom

    # NPV
    npv = calculate_npv(annual_cash_flow, total_capex, params['wacc'], params['lifetime_years'])
    payback = calculate_payback_period(annual_cash_flow, total_capex)
    pi = calculate_profitability_index(annual_cash_flow, total_capex, params['wacc'], params['lifetime_years'])

    return {
        'technology': tech,
        'zone': row['zone'],
        'year': row['year'],
        'capacity_mw': capacity_mw,
        'capex_total_usd': round(total_capex, 0),
        'annual_revenue_usd': round(annual_revenue, 0),
        'annual_fom_usd': round(annual_fom, 0),
        'annual_vom_usd': round(annual_vom, 0),
        'annual_cash_flow_usd': round(annual_cash_flow, 0),
        'npv_usd': round(npv, 0),
        'payback_years': round(payback, 1),
        'profitability_index': round(pi, 2),
        'wacc': params['wacc'],
        'lifetime_years': params['lifetime_years']
    }


def calculate_npv_ccgt(row, tech_params):
    """Calculates NPV for CCGT"""
    tech = 'CCGT'
    params = tech_params[tech_params['tech'] == tech].iloc[0]

    capacity_mw = row['capacity_mw']
    annual_revenue = row['annual_revenue_usd']

    # CAPEX
    total_capex = params['capex_per_kw'] * capacity_mw * 1000

    # Annual costs (FOM only, fuel/vom already in gross margin)
    annual_fom = params['fom_per_kw_yr'] * capacity_mw * 1000

    # Annual cash flow = Gross Margin - FOM
    annual_gross_margin = row['annual_gross_margin_usd']
    annual_cash_flow = annual_gross_margin - annual_fom

    # NPV
    npv = calculate_npv(annual_cash_flow, total_capex, params['wacc'], params['lifetime_years'])
    payback = calculate_payback_period(annual_cash_flow, total_capex)
    pi = calculate_profitability_index(annual_cash_flow, total_capex, params['wacc'], params['lifetime_years'])

    return {
        'technology': 'CCGT',
        'zone': row['zone'],
        'year': row['year'],
        'scenario': row['scenario'],
        'carbon_price': row['carbon_price_usd_per_t'],
        'capacity_mw': capacity_mw,
        'capacity_factor_pct': row['capacity_factor_pct'],
        'capex_total_usd': round(total_capex, 0),
        'annual_revenue_usd': round(annual_revenue, 0),
        'annual_gross_margin_usd': round(annual_gross_margin, 0),
        'annual_fom_usd': round(annual_fom, 0),
        'annual_cash_flow_usd': round(annual_cash_flow, 0),
        'npv_usd': round(npv, 0),
        'payback_years': round(payback, 1),
        'profitability_index': round(pi, 2),
        'wacc': params['wacc'],
        'lifetime_years': params['lifetime_years']
    }


# Calculate NPV for all technologies
print("\n" + "="*70)
print("Calculating NPV and Financial Metrics...")
print("="*70)

results = []

# PV-only
print("\n1. Solar PV:")
for _, row in df_pv.iterrows():
    npv_result = calculate_npv_pv(row, df_tech)
    results.append(npv_result)
    print(f"  {row['zone']} {row['year']}: NPV = ${npv_result['npv_usd']:,.0f}, "
          f"Payback = {npv_result['payback_years']:.1f} years")

# Solar+Storage
print("\n2. Solar+Storage (2h and 4h):")
for _, row in df_storage.iterrows():
    npv_result = calculate_npv_storage(row, df_tech)
    if npv_result:
        results.append(npv_result)
        print(f"  {row['zone']} {row['year']} {row['duration_h']}h: "
              f"NPV = ${npv_result['npv_usd']:,.0f}, "
              f"Payback = {npv_result['payback_years']:.1f} years")

# CCGT (historical scenarios only)
print("\n3. CCGT (historical scenarios):")
df_ccgt_hist = df_ccgt[df_ccgt['carbon_price_usd_per_t'] == 0]
for _, row in df_ccgt_hist.iterrows():
    npv_result = calculate_npv_ccgt(row, df_tech)
    results.append(npv_result)
    print(f"  {row['zone']} {row['year']}: "
          f"NPV = ${npv_result['npv_usd']:,.0f}, "
          f"Payback = {npv_result['payback_years']:.1f} years")

# Create summary DataFrame
df_npv = pd.DataFrame(results)

# Save detailed results
df_npv.to_csv(OUT_DIR / 'npv_results.csv', index=False)
print(f"\nDetailed results saved: npv_results.csv")

# Summary statistics
print("\n" + "="*70)
print("NPV Summary by Technology (2019-2023)")
print("="*70)

npv_summary = df_npv.groupby('technology').agg({
    'npv_usd': ['mean', 'min', 'max'],
    'payback_years': ['mean', 'min', 'max'],
    'profitability_index': ['mean', 'min', 'max']
}).round(2)

print("\n" + npv_summary.to_string())

# Best performing technology by year
print("\n" + "="*70)
print("Best Technology by Year (Highest NPV)")
print("="*70)

for year in sorted(df_npv['year'].unique()):
    year_data = df_npv[df_npv['year'] == year]

    for zone in ['HOUSTON', 'WEST']:
        zone_year = year_data[year_data['zone'] == zone]
        if not zone_year.empty:
            best = zone_year.loc[zone_year['npv_usd'].idxmax()]
            print(f"  {zone} {int(year)}: {best['technology']} "
                  f"(NPV=${best['npv_usd']:,.0f}, Payback={best['payback_years']:.1f} years, PI={best['profitability_index']:.2f})")

"""
MODULE 3.3: Risk-Adjusted NPV Calculation
Risk-adjusted NPV based on historical data - removing extreme event impacts

Input:
  - pv_revenue_results.csv
  - storage_revenue_results.csv
  - ccgt_dispatch_results.csv
  - tech_params.csv

Output:
  - npv_risk_adjusted_results.csv
"""

import pandas as pd
import numpy as np
from pathlib import Path
import warnings
warnings.filterwarnings('ignore')

OUT_DIR = Path('out')
OUT_DIR.mkdir(exist_ok=True)

print("="*70)
print("MODULE 3.3: Risk-Adjusted NPV Calculation")
print("="*70)

# Load parameters
print("\nLoading parameters...")
df_tech = pd.read_csv(OUT_DIR / 'tech_params.csv')

# Load revenue results
print("Loading revenue results...")
df_pv = pd.read_csv(OUT_DIR / 'pv_revenue_results.csv')
df_storage = pd.read_csv(OUT_DIR / 'storage_revenue_results.csv')
df_ccgt = pd.read_csv(OUT_DIR / 'ccgt_dispatch_results.csv')

# Filter historical scenarios only
df_ccgt = df_ccgt[df_ccgt['carbon_price_usd_per_t'] == 0]


def calculate_npv(annual_cash_flow, capex, wacc, lifetime_years):
    """Calculate NPV given annual cash flow"""
    pv_cash_flows = sum([annual_cash_flow / (1 + wacc)**t
                         for t in range(1, lifetime_years + 1)])
    npv = pv_cash_flows - capex
    return npv


def calculate_risk_adjusted_metrics(revenues):

    revenues = np.array(revenues)
    n = len(revenues)

    # 1. Median
    median_rev = np.median(revenues)

    # 2. Trimmed Mean
    if n >= 5:
        sorted_rev = np.sort(revenues)
        trimmed_rev = np.mean(sorted_rev[1:-1])
    else:
        trimmed_rev = np.mean(revenues)

    # 3. Winsorized Mean
    p20 = np.percentile(revenues, 20)
    p80 = np.percentile(revenues, 80)
    winsorized = np.clip(revenues, p20, p80)
    winsorized_rev = np.mean(winsorized)

    # 4. Probability-Weighted
    median = np.median(revenues)
    is_extreme = revenues > 2 * median

    if is_extreme.any():
        extreme_rev = revenues[is_extreme].mean()
        normal_rev = revenues[~is_extreme].mean() if (~is_extreme).any() else median
        prob_weighted_rev = 0.1 * extreme_rev + 0.9 * normal_rev
    else:
        prob_weighted_rev = np.mean(revenues)

    return {
        'median': median_rev,
        'trimmed_mean': trimmed_rev,
        'winsorized_mean': winsorized_rev,
        'prob_weighted': prob_weighted_rev,
        'mean': np.mean(revenues),
        'std': np.std(revenues),
        'min': np.min(revenues),
        'max': np.max(revenues)
    }


print("\n" + "="*70)
print("Calculating Risk-Adjusted NPV...")
print("="*70)

results = []

# Process each technology and zone
technologies = [
    ('Solar_PV', df_pv, 'annual_revenue_usd'),
    ('Solar_PV_Storage_2h', df_storage[df_storage['duration_h'] == 2], 'annual_total_revenue_usd'),
    ('Solar_PV_Storage_4h', df_storage[df_storage['duration_h'] == 4], 'annual_total_revenue_usd'),
    ('CCGT', df_ccgt, 'annual_gross_margin_usd')  # Use gross margin for CCGT
]

for tech_name, df_tech_data, revenue_col in technologies:
    print(f"\n{tech_name}:")

    # Get tech parameters
    if 'CCGT' in tech_name:
        params = df_tech[df_tech['tech'] == 'CCGT'].iloc[0]
    elif '4h' in tech_name:
        params = df_tech[df_tech['tech'] == 'Solar_PV_Storage_4h'].iloc[0]
    elif '2h' in tech_name:
        params = df_tech[df_tech['tech'] == 'Solar_PV_Storage_2h'].iloc[0]
    else:
        params = df_tech[df_tech['tech'] == 'Solar_PV'].iloc[0]

    capacity_mw = 100
    total_capex = params['capex_per_kw'] * capacity_mw * 1000
    annual_fom = params['fom_per_kw_yr'] * capacity_mw * 1000

    for zone in ['HOUSTON', 'WEST']:
        zone_data = df_tech_data[df_tech_data['zone'] == zone]

        if len(zone_data) == 0:
            continue

        # Get revenues/margins across all years
        revenues = zone_data[revenue_col].values

        # Calculate risk metrics
        risk_metrics = calculate_risk_adjusted_metrics(revenues)

        # Calculate NPV for each method
        npv_results = {}

        for method in ['median', 'trimmed_mean', 'winsorized_mean', 'prob_weighted', 'mean']:
            revenue = risk_metrics[method]

            # Annual cash flow = revenue/margin - FOM
            annual_cash_flow = revenue - annual_fom

            # Calculate NPV
            npv = calculate_npv(annual_cash_flow, total_capex, params['wacc'], params['lifetime_years'])
            npv_results[f'npv_{method}'] = round(npv, 0)

        # Store results
        result = {
            'technology': tech_name,
            'zone': zone,
            'n_years': len(revenues),
            'revenue_mean': round(risk_metrics['mean'], 0),
            'revenue_median': round(risk_metrics['median'], 0),
            'revenue_std': round(risk_metrics['std'], 0),
            'revenue_min': round(risk_metrics['min'], 0),
            'revenue_max': round(risk_metrics['max'], 0),
            **npv_results,
            'capex': total_capex,
            'fom': annual_fom,
            'wacc': params['wacc']
        }

        results.append(result)

        print(f"  {zone}:")
        print(f"    Revenue range: ${risk_metrics['min']:,.0f} - ${risk_metrics['max']:,.0f}")
        print(f"    Revenue mean: ${risk_metrics['mean']:,.0f}")
        print(f"    Revenue median: ${risk_metrics['median']:,.0f}")
        print(f"    NPV (median): ${npv_results['npv_median']:,.0f}")
        print(f"    NPV (prob-weighted): ${npv_results['npv_prob_weighted']:,.0f}")

# Create DataFrame
df_results = pd.DataFrame(results)

# Save
df_results.to_csv(OUT_DIR / 'npv_risk_adjusted_results.csv', index=False)
print(f"\nDetailed results saved: npv_risk_adjusted_results.csv")

# Summary comparison
print("\n" + "="*70)
print("NPV Comparison: Different Risk-Adjustment Methods")
print("="*70)

print("\nRecommended Method: MEDIAN (most robust)")
print("-" * 70)

median_summary = df_results.groupby('technology')[['npv_median', 'npv_prob_weighted', 'npv_mean']].mean()
median_summary.columns = ['NPV_Median', 'NPV_ProbWeighted', 'NPV_Mean']
median_summary = median_summary.round(0)

print("\n" + median_summary.to_string())

# Detailed comparison by zone
print("\n" + "="*70)
print("Detailed Comparison by Zone")
print("="*70)

for zone in ['HOUSTON', 'WEST']:
    print(f"\n{zone}:")
    zone_results = df_results[df_results['zone'] == zone]

    for _, row in zone_results.iterrows():
        print(f"\n  {row['technology']}:")
        print(f"    Revenue: Mean=${row['revenue_mean']:,.0f}, Median=${row['revenue_median']:,.0f}")
        print(f"    NPV Methods:")
        print(f"      Median (recommended):  ${row['npv_median']:>15,.0f}")
        print(f"      Prob-Weighted (10%):   ${row['npv_prob_weighted']:>15,.0f}")
        print(f"      Trimmed Mean:          ${row['npv_trimmed_mean']:>15,.0f}")
        print(f"      Winsorized Mean:       ${row['npv_winsorized_mean']:>15,.0f}")
        print(f"      Simple Mean:           ${row['npv_mean']:>15,.0f}")

# Technology ranking by median NPV
print("\n" + "="*70)
print("Technology Ranking (by Median NPV)")
print("="*70)

ranking = df_results.groupby('technology')['npv_median'].mean().sort_values(ascending=False)
print("\nAverage across zones:")
for tech, npv in ranking.items():
    print(f"  {tech:25s}: ${npv:>15,.0f}")

# Key insights
print("\n" + "="*70)
print("Key Insights")
print("="*70)

# Compare CCGT before and after risk adjustment
ccgt_data = df_results[df_results['technology'] == 'CCGT']
ccgt_mean_npv = ccgt_data['npv_mean'].mean()
ccgt_median_npv = ccgt_data['npv_median'].mean()

if ccgt_mean_npv != 0:
    ccgt_reduction = (1 - ccgt_median_npv / ccgt_mean_npv) * 100
else:
    ccgt_reduction = 0

print(f"\nCCGT NPV Adjustment:")
print(f"  Simple mean (includes Uri windfall): ${ccgt_mean_npv:,.0f}")
print(f"  Risk-adjusted (median): ${ccgt_median_npv:,.0f}")
print(f"  Reduction: {ccgt_reduction:.1f}%")

# Winner analysis
print("\nBest Technology (Median NPV):")
for zone in ['HOUSTON', 'WEST']:
    zone_data = df_results[df_results['zone'] == zone]
    best = zone_data.loc[zone_data['npv_median'].idxmax()]
    print(f"  {zone}: {best['technology']} (${best['npv_median']:,.0f})")

# Profitability rate
print("\nProfitability (Median NPV > 0):")
for tech in df_results['technology'].unique():
    tech_data = df_results[df_results['technology'] == tech]
    profitable = (tech_data['npv_median'] > 0).sum()
    total = len(tech_data)
    pct = (profitable / total * 100) if total > 0 else 0
    print(f"  {tech:25s}: {profitable}/{total} zones ({pct:.0f}%)")

# Revenue variability
print("\n" + "="*70)
print("Revenue Variability (CV = Std/Mean)")
print("="*70)

for tech in df_results['technology'].unique():
    tech_data = df_results[df_results['technology'] == tech]
    avg_cv = (tech_data['revenue_std'] / tech_data['revenue_mean']).mean() * 100
    print(f"  {tech:25s}: {avg_cv:.1f}%")

print("\nMODULE 3.3 COMPLETE.")
print("Risk-adjusted NPV calculated using 4 methods")
print("Recommendation: Use MEDIAN for most robust estimates")

"""
MODULE 4: Visualization for Report
Generates publication-quality figures for the analysis report

Output:
  - fig_annual_revenue.png
  - fig_uri_contribution.png
  - fig_lcoe_comparison.png
  - fig_npv_comparison.png
  - fig_risk_adjusted_npv.png
  - fig_ccgt_sensitivity.png
  - fig_storage_value.png
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.ticker as mticker
from pathlib import Path
import warnings
warnings.filterwarnings('ignore')

# Set style
plt.style.use('seaborn-v0_8-whitegrid')
plt.rcParams['figure.figsize'] = (10, 6)
plt.rcParams['font.size'] = 11
plt.rcParams['axes.labelsize'] = 12
plt.rcParams['axes.titlesize'] = 14
plt.rcParams['legend.fontsize'] = 10

OUT_DIR = Path('out')
FIG_DIR = Path('figures')
FIG_DIR.mkdir(exist_ok=True)

# Color palette
COLORS = {
    'Solar_PV': '#F4B942',           # Gold
    'Solar_PV_Storage_2h': '#4ECDC4', # Teal
    'Solar_PV_Storage_4h': '#1A535C', # Dark teal
    'CCGT': '#E63946',                # Red
    'HOUSTON': '#457B9D',             # Blue
    'WEST': '#2A9D8F'                 # Green
}

print("="*70)
print("MODULE 4: Visualization for Report")
print("="*70)

# Load all results
print("\nLoading results...")
df_pv = pd.read_csv(OUT_DIR / 'pv_revenue_results.csv')
df_storage = pd.read_csv(OUT_DIR / 'storage_revenue_results.csv')
df_ccgt = pd.read_csv(OUT_DIR / 'ccgt_dispatch_results.csv')
df_ccgt = df_ccgt[df_ccgt['carbon_price_usd_per_t'] == 0]  # Historical only

df_lcoe = pd.read_csv(OUT_DIR / 'lcoe_results.csv')
df_npv = pd.read_csv(OUT_DIR / 'npv_results.csv')
df_risk = pd.read_csv(OUT_DIR / 'npv_risk_adjusted_results.csv')

# ============================================================
# Figure 1: Annual Revenue/Margin Comparison
# ============================================================
print("\nGenerating Figure 1: Annual Revenue/Margin...")

fig, axes = plt.subplots(1, 2, figsize=(14, 6))

for idx, zone in enumerate(['HOUSTON', 'WEST']):
    ax = axes[idx]

    years = [2019, 2020, 2021, 2022, 2023]
    x = np.arange(len(years))
    width = 0.2

    # Get data
    pv_rev = df_pv[df_pv['zone'] == zone]['annual_revenue_usd'].values / 1e6
    storage_4h = df_storage[(df_storage['zone'] == zone) & (df_storage['duration_h'] == 4)]['annual_total_revenue_usd'].values / 1e6
    ccgt_margin = df_ccgt[df_ccgt['zone'] == zone]['annual_gross_margin_usd'].values / 1e6

    # Plot bars
    bars1 = ax.bar(x - width, pv_rev, width, label='Solar PV', color=COLORS['Solar_PV'], edgecolor='white')
    bars2 = ax.bar(x, storage_4h, width, label='Solar+Storage 4h', color=COLORS['Solar_PV_Storage_4h'], edgecolor='white')
    bars3 = ax.bar(x + width, ccgt_margin, width, label='CCGT', color=COLORS['CCGT'], edgecolor='white')

    ax.set_xlabel('Year')
    ax.set_ylabel('Annual Revenue / Margin ($ millions)')
    ax.set_title(f'{zone} Zone')
    ax.set_xticks(x)
    ax.set_xticklabels(years)
    ax.legend(loc='upper left')

    # Add value labels on 2021 bars
    for bars in [bars1, bars2, bars3]:
        bar = bars[2]  # 2021 is index 2
        height = bar.get_height()
        if height > 50:
            ax.annotate(f'${height:.0f}M',
                       xy=(bar.get_x() + bar.get_width()/2, height),
                       ha='center', va='bottom', fontsize=9, fontweight='bold')

plt.suptitle('Annual Revenue (Solar) vs Gross Margin (CCGT) by Zone', fontsize=14, fontweight='bold')
plt.tight_layout()
plt.savefig(FIG_DIR / 'fig_annual_revenue.png', dpi=300, bbox_inches='tight')
plt.savefig(FIG_DIR / 'fig_annual_revenue.pdf', bbox_inches='tight')
print(f"  Saved: fig_annual_revenue.png")
plt.close()

# ============================================================
# Figure 2: Winter Storm Uri Contribution
# ============================================================
print("\nGenerating Figure 2: Uri Contribution...")

fig, axes = plt.subplots(1, 2, figsize=(12, 5))

# Data for Uri contribution
uri_data = {
    'Technology': ['Solar PV\nHouston', 'Solar PV\nWest', 'Solar+4h\nHouston', 'Solar+4h\nWest', 'CCGT\nHouston', 'CCGT\nWest'],
    'Uri_pct': [61, 75, 51, 50, 92, 93],  # Uri % of annual
    'Non_Uri_pct': [39, 25, 49, 50, 8, 7]
}

# Left: Stacked bar showing Uri vs Non-Uri
ax = axes[0]
x = np.arange(len(uri_data['Technology']))
width = 0.6

colors_uri = ['#E63946', '#E63946', '#1A535C', '#1A535C', '#457B9D', '#457B9D']
bars1 = ax.bar(x, uri_data['Uri_pct'], width, label='Uri Period (Feb 12-20)', color='#E63946', alpha=0.8)
bars2 = ax.bar(x, uri_data['Non_Uri_pct'], width, bottom=uri_data['Uri_pct'], label='Rest of 2021', color='#457B9D', alpha=0.6)

ax.set_ylabel('Percentage of Annual Revenue/Margin')
ax.set_title('Winter Storm Uri Contribution to 2021 Results')
ax.set_xticks(x)
ax.set_xticklabels(uri_data['Technology'], fontsize=9)
ax.legend(loc='upper right')
ax.set_ylim(0, 110)

# Add percentage labels
for i, (bar, pct) in enumerate(zip(bars1, uri_data['Uri_pct'])):
    ax.annotate(f'{pct}%', xy=(bar.get_x() + bar.get_width()/2, pct/2),
               ha='center', va='center', fontsize=10, fontweight='bold', color='white')

# Right: Timeline of prices during Uri
ax2 = axes[1]

# Load hourly data for Uri period
try:
    df_uri_prices = pd.read_parquet(OUT_DIR / 'ercot_dam_HOUSTON_2021.parquet')
    dt_col = 'datetime' if 'datetime' in df_uri_prices.columns else 'datetime_local'
    df_uri_prices['date'] = pd.to_datetime(df_uri_prices[dt_col]).dt.date

    # Filter Uri period
    uri_start = pd.Timestamp('2021-02-12').date()
    uri_end = pd.Timestamp('2021-02-20').date()
    df_uri = df_uri_prices[(df_uri_prices['date'] >= uri_start) & (df_uri_prices['date'] <= uri_end)]

    ax2.plot(range(len(df_uri)), df_uri['price_usd_per_mwh'].values, color='#E63946', linewidth=0.8)
    ax2.axhline(y=9000, color='black', linestyle='--', linewidth=1, label='Price Cap ($9,000/MWh)')
    ax2.axhline(y=407, color='#457B9D', linestyle=':', linewidth=1.5, label='CCGT Marginal Cost ($407/MWh)')

    ax2.set_xlabel('Hours (Feb 12-20, 2021)')
    ax2.set_ylabel('Day-Ahead Price ($/MWh)')
    ax2.set_title('ERCOT Houston Prices During Winter Storm Uri')
    ax2.legend(loc='upper right')
    ax2.set_ylim(0, 10000)
    ax2.yaxis.set_major_formatter(mticker.StrMethodFormatter('${x:,.0f}'))
except Exception as e:
    ax2.text(0.5, 0.5, f'Price data not available\n{e}', ha='center', va='center', transform=ax2.transAxes)

plt.tight_layout()
plt.savefig(FIG_DIR / 'fig_uri_contribution.png', dpi=300, bbox_inches='tight')
plt.savefig(FIG_DIR / 'fig_uri_contribution.pdf', bbox_inches='tight')
print(f"  Saved: fig_uri_contribution.png")
plt.close()

# ============================================================
# Figure 3: LCOE Comparison
# ============================================================
print("\nGenerating Figure 3: LCOE Comparison...")

fig, ax = plt.subplots(figsize=(10, 6))

# Calculate average LCOE by tech and zone
techs = ['Solar_PV', 'Solar_PV_Storage_2h', 'Solar_PV_Storage_4h', 'CCGT']
tech_labels = ['Solar PV', 'Solar+2h', 'Solar+4h', 'CCGT']

x = np.arange(len(techs))
width = 0.35

lcoe_houston = []
lcoe_west = []

for tech in techs:
    tech_data = df_lcoe[df_lcoe['technology'] == tech]
    lcoe_houston.append(tech_data[tech_data['zone'] == 'HOUSTON']['lcoe_usd_per_mwh'].mean())
    lcoe_west.append(tech_data[tech_data['zone'] == 'WEST']['lcoe_usd_per_mwh'].mean())

bars1 = ax.bar(x - width/2, lcoe_houston, width, label='Houston', color=COLORS['HOUSTON'])
bars2 = ax.bar(x + width/2, lcoe_west, width, label='West', color=COLORS['WEST'])

ax.set_ylabel('Levelized Cost of Energy ($/MWh)')
ax.set_title('LCOE Comparison by Technology and Zone (2019-2023 Average)')
ax.set_xticks(x)
ax.set_xticklabels(tech_labels)
ax.legend()

# Add value labels
for bars in [bars1, bars2]:
    for bar in bars:
        height = bar.get_height()
        ax.annotate(f'${height:.0f}',
                   xy=(bar.get_x() + bar.get_width()/2, height),
                   ha='center', va='bottom', fontsize=9)

ax.set_ylim(0, 140)
plt.tight_layout()
plt.savefig(FIG_DIR / 'fig_lcoe_comparison.png', dpi=300, bbox_inches='tight')
plt.savefig(FIG_DIR / 'fig_lcoe_comparison.pdf', bbox_inches='tight')
print(f"  Saved: fig_lcoe_comparison.png")
plt.close()

# ============================================================
# Figure 4: Risk-Adjusted NPV Comparison
# ============================================================
print("\nGenerating Figure 4: Risk-Adjusted NPV...")

fig, axes = plt.subplots(1, 2, figsize=(14, 6))

# Left: Bar chart of NPV by method
ax = axes[0]

techs = ['CCGT', 'Solar_PV_Storage_4h', 'Solar_PV_Storage_2h', 'Solar_PV']
tech_labels = ['CCGT', 'Solar+4h', 'Solar+2h', 'Solar PV']

npv_median = []
npv_mean = []

for tech in techs:
    tech_data = df_risk[df_risk['technology'] == tech]
    npv_median.append(tech_data['npv_median'].mean() / 1e6)
    npv_mean.append(tech_data['npv_mean'].mean() / 1e6)

x = np.arange(len(techs))
width = 0.35

bars1 = ax.bar(x - width/2, npv_mean, width, label='Simple Mean NPV', color='#E63946', alpha=0.6)
bars2 = ax.bar(x + width/2, npv_median, width, label='Median NPV (Risk-Adjusted)', color='#1A535C')

ax.set_ylabel('Net Present Value ($ millions)')
ax.set_title('NPV: Simple Mean vs Risk-Adjusted')
ax.set_xticks(x)
ax.set_xticklabels(tech_labels)
ax.legend()
ax.axhline(y=0, color='black', linestyle='-', linewidth=0.5)

# Add value labels
for bar in bars2:
    height = bar.get_height()
    ax.annotate(f'${height:.0f}M',
               xy=(bar.get_x() + bar.get_width()/2, height),
               ha='center', va='bottom' if height > 0 else 'top', fontsize=9, fontweight='bold')

# Right: Box plot of annual cash flows
ax2 = axes[1]

# Prepare data for box plot
cash_flow_data = []
labels = []

for tech, label in zip(['Solar_PV', 'Solar_PV_Storage_4h', 'CCGT'], ['Solar PV', 'Solar+4h', 'CCGT']):
    if tech == 'Solar_PV':
        vals = df_pv['annual_revenue_usd'].values / 1e6
    elif tech == 'Solar_PV_Storage_4h':
        vals = df_storage[df_storage['duration_h'] == 4]['annual_total_revenue_usd'].values / 1e6
    else:
        vals = df_ccgt['annual_gross_margin_usd'].values / 1e6
    cash_flow_data.append(vals)
    labels.append(label)

bp = ax2.boxplot(cash_flow_data, labels=labels, patch_artist=True)

colors_box = [COLORS['Solar_PV'], COLORS['Solar_PV_Storage_4h'], COLORS['CCGT']]
for patch, color in zip(bp['boxes'], colors_box):
    patch.set_facecolor(color)
    patch.set_alpha(0.7)

ax2.set_ylabel('Annual Revenue/Margin ($ millions)')
ax2.set_title('Revenue/Margin Distribution (2019-2023)')

# Mark Uri outlier
ax2.annotate('Uri 2021', xy=(3, 98), ha='center', fontsize=9, color='#E63946')

plt.tight_layout()
plt.savefig(FIG_DIR / 'fig_npv_comparison.png', dpi=300, bbox_inches='tight')
plt.savefig(FIG_DIR / 'fig_npv_comparison.pdf', bbox_inches='tight')
print(f"  Saved: fig_npv_comparison.png")
plt.close()

# ============================================================
# Figure 5: Storage Value Over Time
# ============================================================
print("\nGenerating Figure 5: Storage Value...")

fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# Left: Storage revenue as % of PV revenue
ax = axes[0]

years = [2019, 2020, 2021, 2022, 2023]

for zone, color in [('HOUSTON', COLORS['HOUSTON']), ('WEST', COLORS['WEST'])]:
    pv_rev = df_pv[df_pv['zone'] == zone]['annual_revenue_usd'].values
    storage_rev = df_storage[(df_storage['zone'] == zone) & (df_storage['duration_h'] == 4)]['annual_storage_revenue_usd'].values
    storage_gain = (storage_rev / pv_rev) * 100

    ax.plot(years, storage_gain, 'o-', color=color, label=f'{zone}', linewidth=2, markersize=8)

ax.set_xlabel('Year')
ax.set_ylabel('Storage Revenue as % of PV Revenue')
ax.set_title('4-Hour Battery Arbitrage Value Relative to Solar PV')
ax.legend()
ax.axhline(y=100, color='gray', linestyle='--', linewidth=1, alpha=0.5)
ax.set_ylim(0, 250)

# Right: 2h vs 4h comparison
ax2 = axes[1]

x = np.arange(len(years))
width = 0.35

storage_2h_avg = []
storage_4h_avg = []

for year in years:
    s2h = df_storage[(df_storage['year'] == year) & (df_storage['duration_h'] == 2)]['annual_storage_revenue_usd'].mean() / 1e6
    s4h = df_storage[(df_storage['year'] == year) & (df_storage['duration_h'] == 4)]['annual_storage_revenue_usd'].mean() / 1e6
    storage_2h_avg.append(s2h)
    storage_4h_avg.append(s4h)

bars1 = ax2.bar(x - width/2, storage_2h_avg, width, label='2-Hour Battery', color=COLORS['Solar_PV_Storage_2h'])
bars2 = ax2.bar(x + width/2, storage_4h_avg, width, label='4-Hour Battery', color=COLORS['Solar_PV_Storage_4h'])

ax2.set_xlabel('Year')
ax2.set_ylabel('Storage Arbitrage Revenue ($ millions)')
ax2.set_title('Battery Duration Comparison: 2h vs 4h')
ax2.set_xticks(x)
ax2.set_xticklabels(years)
ax2.legend()

plt.tight_layout()
plt.savefig(FIG_DIR / 'fig_storage_value.png', dpi=300, bbox_inches='tight')
plt.savefig(FIG_DIR / 'fig_storage_value.pdf', bbox_inches='tight')
print(f"  Saved: fig_storage_value.png")
plt.close()

# ============================================================
# Figure 6: CCGT Capacity Factor vs Gas Price
# ============================================================
print("\nGenerating Figure 6: CCGT Sensitivity")

fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# Left: CF vs Gas Price
ax = axes[0]

for zone, color, marker in [('HOUSTON', COLORS['HOUSTON'], 'o'), ('WEST', COLORS['WEST'], 's')]:
    zone_data = df_ccgt[df_ccgt['zone'] == zone]
    gas_prices = zone_data['avg_gas_price_when_running'].values
    cf = zone_data['capacity_factor_pct'].values
    years_label = zone_data['year'].values

    ax.scatter(gas_prices, cf, c=color, s=100, marker=marker, label=zone, alpha=0.8)

    # Label each point with year
    for gp, c, yr in zip(gas_prices, cf, years_label):
        ax.annotate(f'{int(yr)}', xy=(gp, c), xytext=(5, 5), textcoords='offset points', fontsize=8)

ax.set_xlabel('Average Gas Price When Running ($/MMBtu)')
ax.set_ylabel('Capacity Factor (%)')
ax.set_title('CCGT Capacity Factor vs Gas Price')
ax.legend()

# Right: Gross Margin vs CF
ax2 = axes[1]

for zone, color, marker in [('HOUSTON', COLORS['HOUSTON'], 'o'), ('WEST', COLORS['WEST'], 's')]:
    zone_data = df_ccgt[df_ccgt['zone'] == zone]
    cf = zone_data['capacity_factor_pct'].values
    margin = zone_data['annual_gross_margin_usd'].values / 1e6
    years_label = zone_data['year'].values

    ax2.scatter(cf, margin, c=color, s=100, marker=marker, label=zone, alpha=0.8)

    for c, m, yr in zip(cf, margin, years_label):
        ax2.annotate(f'{int(yr)}', xy=(c, m), xytext=(5, 5), textcoords='offset points', fontsize=8)

ax2.set_xlabel('Capacity Factor (%)')
ax2.set_ylabel('Annual Gross Margin ($ millions)')
ax2.set_title('CCGT Gross Margin vs Capacity Factor')
ax2.legend()

# Add annotation for 2021 outlier
ax2.annotate('Uri windfall', xy=(37, 98), xytext=(50, 80),
            arrowprops=dict(arrowstyle='->', color='gray'),
            fontsize=9, color='#E63946')

plt.tight_layout()
plt.savefig(FIG_DIR / 'fig_ccgt_sensitivity.png', dpi=300, bbox_inches='tight')
plt.savefig(FIG_DIR / 'fig_ccgt_sensitivity.pdf', bbox_inches='tight')
print(f"  Saved: fig_ccgt_sensitivity.png")
plt.close()

# ============================================================
# Figure 7: Value Factor Trend (Solar Cannibalization)
# ============================================================
print("\nGenerating Figure 7: Value Factor Trend...")

fig, ax = plt.subplots(figsize=(10, 6))

years = [2019, 2020, 2021, 2022, 2023]

for zone, color in [('HOUSTON', COLORS['HOUSTON']), ('WEST', COLORS['WEST'])]:
    zone_data = df_pv[df_pv['zone'] == zone].sort_values('year')
    vf = zone_data['value_factor_pct'].values

    ax.plot(years, vf, 'o-', color=color, label=zone, linewidth=2, markersize=10)

ax.axhline(y=100, color='gray', linestyle='--', linewidth=1, label='Market Average (100%)')
ax.set_xlabel('Year')
ax.set_ylabel('Value Factor (%)')
ax.set_title('Solar PV Value Factor Trend (2019-2023)\n"Duck Curve" Effect: Solar Captures Lower Prices as Penetration Increases')
ax.legend()
ax.set_ylim(60, 160)

# Add annotation
ax.annotate('Solar cannibalization\neffect emerging', xy=(2023, 85), ha='center', fontsize=9,
           bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))

plt.tight_layout()
plt.savefig(FIG_DIR / 'fig_value_factor.png', dpi=300, bbox_inches='tight')
plt.savefig(FIG_DIR / 'fig_value_factor.pdf', bbox_inches='tight')
print(f"  Saved: fig_value_factor.png")
plt.close()

# ============================================================
# Figure 8: Technology Ranking Summary
# ============================================================
print("\nGenerating Figure 8: Technology Ranking...")

fig, ax = plt.subplots(figsize=(10, 6))

# Data
techs = ['CCGT', 'Solar+Storage 4h', 'Solar+Storage 2h', 'Solar PV']
npv_values = [159, 81, 56, -7]
colors = [COLORS['CCGT'], COLORS['Solar_PV_Storage_4h'], COLORS['Solar_PV_Storage_2h'], COLORS['Solar_PV']]

bars = ax.barh(techs, npv_values, color=colors, edgecolor='white', height=0.6)

ax.axvline(x=0, color='black', linewidth=1)
ax.set_xlabel('Risk-Adjusted NPV ($ millions)')
ax.set_title('Technology Ranking by Median NPV\n(100 MW Systems, 2019-2023 Historical Data)')

# Add value labels
for bar, val in zip(bars, npv_values):
    x_pos = val + 5 if val > 0 else val - 5
    ha = 'left' if val > 0 else 'right'
    ax.annotate(f'${val}M', xy=(x_pos, bar.get_y() + bar.get_height()/2),
               ha=ha, va='center', fontsize=11, fontweight='bold')

ax.set_xlim(-50, 200)
plt.tight_layout()
plt.savefig(FIG_DIR / 'fig_technology_ranking.png', dpi=300, bbox_inches='tight')
plt.savefig(FIG_DIR / 'fig_technology_ranking.pdf', bbox_inches='tight')
print(f"  Saved: fig_technology_ranking.png")
plt.close()

print("\n" + "="*70)
print("MODULE 4 COMPLETE.")
print(f"Generated 8 figures in {FIG_DIR}/")
print("="*70)

"""MODULE 5: Sensitivity Analysis"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from pathlib import Path

OUT_DIR = Path('out')
FIG_DIR = Path('figures')
FIG_DIR.mkdir(exist_ok=True)

plt.style.use('seaborn-v0_8-whitegrid')
plt.rcParams['figure.figsize'] = (12, 6)
plt.rcParams['font.size'] = 11

# Load results
df_ccgt = pd.read_csv(OUT_DIR / 'ccgt_dispatch_results.csv')
df_ccgt = df_ccgt[df_ccgt['carbon_price_usd_per_t'] == 0]

df_storage = pd.read_csv(OUT_DIR / 'storage_revenue_results.csv')
df_pv = pd.read_csv(OUT_DIR / 'pv_revenue_results.csv')

# Financial parameters
WACC = 0.065
LIFETIME = 30

# Technology costs (baseline)
CAPEX_CCGT = 1400 * 100 * 1000      # $140M
FOM_CCGT = 17.75 * 100 * 1000       # $1.775M/yr

CAPEX_PV = 1375 * 100 * 1000        # $137.5M
FOM_PV = 12.5 * 100 * 1000          # $1.25M/yr

CAPEX_BATT_PER_KWH = 250            # $/kWh
FOM_BATT = 20 * 100 * 1000          # $2M/yr

def pv_factor(wacc, lifetime):
    return sum([1/(1+wacc)**t for t in range(1, lifetime+1)])

def calculate_npv(annual_revenue, annual_fom, capex, wacc=WACC, lifetime=LIFETIME):
    annual_cf = annual_revenue - annual_fom
    return annual_cf * pv_factor(wacc, lifetime) - capex

print("="*70)
print("SENSITIVITY ANALYSIS: Finding Technology Ranking Breakpoints")
print("="*70)

# Get median annual revenues (our baseline risk-adjusted estimate)
ccgt_median_margin = df_ccgt.groupby('zone')['annual_gross_margin_usd'].median()
storage_4h_median_rev = df_storage[df_storage['duration_h'] == 4].groupby('zone')['annual_total_revenue_usd'].median()
pv_median_rev = df_pv.groupby('zone')['annual_revenue_usd'].median()

print("\nBaseline Median Annual Revenue/Margin:")
print(f"  CCGT Houston: ${ccgt_median_margin['HOUSTON']/1e6:.1f}M")
print(f"  CCGT West: ${ccgt_median_margin['WEST']/1e6:.1f}M")
print(f"  Solar+4h Houston: ${storage_4h_median_rev['HOUSTON']/1e6:.1f}M")
print(f"  Solar+4h West: ${storage_4h_median_rev['WEST']/1e6:.1f}M")

# ============================================================
# SENSITIVITY 1: Battery Cost Reduction
# Question: How much must battery costs fall for Solar+Storage to beat CCGT?
# ============================================================
print("\n" + "="*70)
print("SENSITIVITY 1: Battery Cost Reduction")
print("Question: At what battery cost does Solar+4h beat CCGT?")
print("="*70)

battery_costs = np.arange(50, 301, 25)  # $/kWh range
results_batt = []

for zone in ['HOUSTON', 'WEST']:
    ccgt_npv = calculate_npv(ccgt_median_margin[zone], FOM_CCGT, CAPEX_CCGT)

    for batt_cost in battery_costs:
        # Solar+4h CAPEX = PV + Battery
        capex_storage = CAPEX_PV + batt_cost * 100 * 1000 * 4  # 100MW * 4h = 400MWh
        fom_storage = FOM_PV + FOM_BATT

        storage_npv = calculate_npv(storage_4h_median_rev[zone], fom_storage, capex_storage)

        results_batt.append({
            'zone': zone,
            'battery_cost_per_kwh': batt_cost,
            'ccgt_npv_M': ccgt_npv / 1e6,
            'storage_npv_M': storage_npv / 1e6,
            'storage_wins': storage_npv > ccgt_npv
        })

df_batt = pd.DataFrame(results_batt)

# Find breakeven point
for zone in ['HOUSTON', 'WEST']:
    zone_data = df_batt[df_batt['zone'] == zone]
    breakeven_point = zone_data[zone_data['storage_wins']]['battery_cost_per_kwh'].max()
    if pd.isna(breakeven_point):
        print(f"\n{zone}: Solar+4h never beats CCGT in tested range")
    else:
        print(f"\n{zone}: Solar+4h beats CCGT when battery cost < ${breakeven_point}/kWh")
        print(f"  Current assumption: $250/kWh")
        print(f"  Required reduction: {(250-breakeven_point)/250*100:.0f}%")

# Plot
fig, ax = plt.subplots(figsize=(10, 6))

for zone, color, marker in [('HOUSTON', '#457B9D', 'o'), ('WEST', '#2A9D8F', 's')]:
    zone_data = df_batt[df_batt['zone'] == zone]
    ax.plot(zone_data['battery_cost_per_kwh'], zone_data['storage_npv_M'],
            f'{marker}-', color=color, label=f'Solar+4h {zone}', linewidth=2)
    ax.axhline(y=zone_data['ccgt_npv_M'].iloc[0], color=color, linestyle='--', # Use .iloc[0] since it's a constant value per zone
               alpha=0.7, label=f'CCGT {zone} (reference)')

ax.axvline(x=250, color='red', linestyle=':', alpha=0.5, label='Current battery cost ($250/kWh)')
ax.set_xlabel('Battery Cost ($/kWh)')
ax.set_ylabel('Risk-Adjusted NPV ($ millions)')
ax.set_title('Sensitivity: When Does Solar+Storage Beat CCGT?\n(Breakeven Analysis: Battery Cost)')
ax.legend(loc='upper right')
ax.grid(True, alpha=0.3)

# Add annotation for breakeven
ax.annotate('Breakeven zone', xy=(100, 150), fontsize=10,
            bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))

plt.tight_layout()
plt.savefig(FIG_DIR / 'fig_sensitivity_battery_breakeven.png', dpi=300, bbox_inches='tight')
plt.savefig(FIG_DIR / 'fig_sensitivity_battery_breakeven.pdf', bbox_inches='tight')
print(f"\nSaved: fig_sensitivity_battery_breakeven.png")
plt.close()

# ============================================================
# SENSITIVITY 2: Gas Price Impact on CCGT Competitiveness
# Question: At what sustained gas price does CCGT lose its advantage?
# ============================================================
print("\n" + "="*70)
print("SENSITIVITY 2: Natural Gas Price Sensitivity")
print("Question: At what gas price does CCGT lose its advantage?")
print("="*70)

# We need to recalculate CCGT margin at different gas prices
# Simplified approach: use linear relationship between gas price and margin

# Get baseline data
baseline_gas_price = df_ccgt.groupby('zone')['avg_gas_price_when_running'].median()
print(f"\nBaseline median gas price (when running):")
print(f"  HOUSTON: ${baseline_gas_price['HOUSTON']:.2f}/MMBtu")
print(f"  WEST: ${baseline_gas_price['WEST']:.2f}/MMBtu")

# Heat rate and VOM
HEAT_RATE = 6.5125  # MMBtu/MWh
VOM = 3.875         # $/MWh

# Estimate: For each $1 increase in gas price, marginal cost increases by $6.5/MWh
# This reduces the spread and thus the margin

# Get median generation
ccgt_median_gen = df_ccgt.groupby('zone')['annual_generation_mwh'].median()

gas_prices = np.arange(2.0, 10.1, 0.5)  # $/MMBtu
results_gas = []

for zone in ['HOUSTON', 'WEST']:
    baseline_margin = ccgt_median_margin[zone]
    baseline_fuel_cost = baseline_gas_price[zone] * HEAT_RATE * ccgt_median_gen[zone]
    baseline_revenue = baseline_margin + baseline_fuel_cost + VOM * ccgt_median_gen[zone]

    storage_npv = calculate_npv(storage_4h_median_rev[zone], FOM_PV + FOM_BATT,
                                CAPEX_PV + CAPEX_BATT_PER_KWH * 400 * 1000)

    for gas_price in gas_prices:
        # Recalculate fuel cost
        new_fuel_cost = gas_price * HEAT_RATE * ccgt_median_gen[zone]

        # Assume revenue stays same (price-taker), margin changes
        new_margin = baseline_revenue - new_fuel_cost - VOM * ccgt_median_gen[zone]
        new_margin = max(0, new_margin)  # Can't be negative (would shut down)

        ccgt_npv = calculate_npv(new_margin, FOM_CCGT, CAPEX_CCGT)

        results_gas.append({
            'zone': zone,
            'gas_price': gas_price,
            'ccgt_margin_M': new_margin / 1e6,
            'ccgt_npv_M': ccgt_npv / 1e6,
            'storage_npv_M': storage_npv / 1e6,
            'ccgt_wins': ccgt_npv > storage_npv
        })

df_gas = pd.DataFrame(results_gas)

# Find breakeven
for zone in ['HOUSTON', 'WEST']:
    zone_data = df_gas[df_gas['zone'] == zone]
    crossover = zone_data[~zone_data['ccgt_wins']]['gas_price'].min()
    if pd.isna(crossover):
        print(f"\n{zone}: CCGT remains better than Solar+4h across all gas prices tested")
    else:
        print(f"\n{zone}: Solar+4h beats CCGT when gas price > ${crossover:.1f}/MMBtu")
        print(f"  Current median: ${baseline_gas_price[zone]:.2f}/MMBtu")

# Plot
fig, ax = plt.subplots(figsize=(10, 6))

for zone, color in [('HOUSTON', '#457B9D'), ('WEST', '#2A9D8F')]:
    zone_data = df_gas[df_gas['zone'] == zone]
    ax.plot(zone_data['gas_price'], zone_data['ccgt_npv_M'],
            'o-', color=color, label=f'CCGT {zone}', linewidth=2)
    ax.axhline(y=zone_data['storage_npv_M'].iloc[0], color=color, linestyle='--',
               alpha=0.7, label=f'Solar+4h {zone} (reference)')

ax.axvline(x=baseline_gas_price.mean(), color='gray', linestyle=':', alpha=0.5,
           label=f'Historical median (${baseline_gas_price.mean():.1f}/MMBtu)')
ax.set_xlabel('Natural Gas Price ($/MMBtu)')
ax.set_ylabel('Risk-Adjusted NPV ($ millions)')
ax.set_title('Sensitivity: CCGT Profitability vs Gas Price\n(At what price does Solar+Storage become more attractive?)')
ax.legend(loc='upper right')
ax.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig(FIG_DIR / 'fig_sensitivity_gas_breakeven.png', dpi=300, bbox_inches='tight')
plt.savefig(FIG_DIR / 'fig_sensitivity_gas_breakeven.pdf', bbox_inches='tight')
print(f"\nSaved: fig_sensitivity_gas_breakeven.png")
plt.close()

# ============================================================
# SENSITIVITY 3: Extreme Event Frequency
# Question: How many "Uri-like" events per decade change the ranking?
# ============================================================
print("\n" + "="*70)
print("SENSITIVITY 3: Extreme Event Frequency")
print("Question: How does extreme event frequency affect technology choice?")
print("="*70)

# Calculate "normal year" revenue (excluding 2021)
ccgt_normal_margin = df_ccgt[df_ccgt['year'] != 2021].groupby('zone')['annual_gross_margin_usd'].median()

# Get 2021 (Uri) year revenue for each zone by filtering directly
df_ccgt_2021_filtered = df_ccgt[df_ccgt['year'] == 2021]
ccgt_uri_margin = df_ccgt_2021_filtered.set_index('zone')['annual_gross_margin_usd']

storage_normal_rev = df_storage[(df_storage['duration_h'] == 4) & (df_storage['year'] != 2021)].groupby('zone')['annual_total_revenue_usd'].median()

# Get 2021 (Uri) year revenue for Solar+Storage by filtering directly
df_storage_2021_filtered = df_storage[(df_storage['duration_h'] == 4) & (df_storage['year'] == 2021)]
storage_uri_rev = df_storage_2021_filtered.set_index('zone')['annual_total_revenue_usd']

print(f"\nNormal year (median excl. 2021):")
print(f"  CCGT Houston: ${ccgt_normal_margin['HOUSTON']/1e6:.1f}M")
print(f"  Solar+4h Houston: ${storage_normal_rev['HOUSTON']/1e6:.1f}M")

print(f"\nUri year (2021):")
print(f"  CCGT Houston: ${ccgt_uri_margin['HOUSTON']/1e6:.1f}M")
print(f"  Solar+4h Houston: ${storage_uri_rev['HOUSTON']/1e6:.1f}M")

# Simulate different Uri frequencies (events per 30 years)
uri_frequencies = np.arange(0, 11, 1)  # 0 to 10 events in 30 years
results_freq = []

for zone in ['HOUSTON', 'WEST']:
    for n_uri in uri_frequencies:
        # Expected annual revenue = weighted average
        n_normal = 30 - n_uri

        ccgt_expected = (n_normal * ccgt_normal_margin[zone] + n_uri * ccgt_uri_margin[zone]) / 30
        storage_expected = (n_normal * storage_normal_rev[zone] + n_uri * storage_uri_rev[zone]) / 30

        ccgt_npv = calculate_npv(ccgt_expected, FOM_CCGT, CAPEX_CCGT)
        storage_npv = calculate_npv(storage_expected, FOM_PV + FOM_BATT,
                                    CAPEX_PV + CAPEX_BATT_PER_KWH * 400 * 1000)

        results_freq.append({
            'zone': zone,
            'uri_events_per_30yr': n_uri,
            'ccgt_npv_M': ccgt_npv / 1e6,
            'storage_npv_M': storage_npv / 1e6,
            'ccgt_advantage_M': (ccgt_npv - storage_npv) / 1e6
        })

df_freq = pd.DataFrame(results_freq)

print("\nCCGT Advantage (NPV) by Uri Frequency:")
print(df_freq[df_freq['zone'] == 'HOUSTON'][['uri_events_per_30yr', 'ccgt_npv_M', 'storage_npv_M', 'ccgt_advantage_M']].to_string(index=False))

# Plot
fig, ax = plt.subplots(figsize=(10, 6))

for zone, color in [('HOUSTON', '#457B9D'), ('WEST', '#2A9D8F')]:
    zone_data = df_freq[df_freq['zone'] == zone]
    ax.plot(zone_data['uri_events_per_30yr'], zone_data['ccgt_npv_M'],
            'o-', color=color, label=f'CCGT {zone}', linewidth=2)
    ax.plot(zone_data['uri_events_per_30yr'], zone_data['storage_npv_M'],
            's--', color=color, alpha=0.7, label=f'Solar+4h {zone}', linewidth=2)

ax.axvline(x=1, color='red', linestyle=':', alpha=0.5, label='Historical (1 event in 5 years \u2248 6 per 30yr)')
ax.set_xlabel('Number of Uri-like Extreme Events per 30 Years')
ax.set_ylabel('NPV ($ millions)')
ax.set_title('Sensitivity: How Extreme Event Frequency Affects Technology Choice\n(Both technologies benefit, but CCGT benefits more)')
ax.legend(loc='upper left')
ax.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig(FIG_DIR / 'fig_sensitivity_extreme_frequency.png', dpi=300, bbox_inches='tight')
plt.savefig(FIG_DIR / 'fig_sensitivity_extreme_frequency.pdf', bbox_inches='tight')
print(f"\nSaved: fig_sensitivity_extreme_frequency.png")
plt.close()



# ============================================================
# FINAL COMPREHENSIVE FIGURE: Tornado Chart
# ============================================================
print("\n" + "="*70)
print("Creating Tornado Chart: Parameter Sensitivity Summary")
print("="*70)

# Calculate NPV change for +/- variation in each parameter
baseline_ccgt_npv = calculate_npv(ccgt_median_margin['HOUSTON'], FOM_CCGT, CAPEX_CCGT) / 1e6

# Parameter variations
params = {
    'Gas Price\n(\u00b1$2/MMBtu)': {
        'low': calculate_npv(ccgt_median_margin['HOUSTON'] + 2*HEAT_RATE*ccgt_median_gen['HOUSTON'],
                            FOM_CCGT, CAPEX_CCGT) / 1e6,
        'high': calculate_npv(ccgt_median_margin['HOUSTON'] - 2*HEAT_RATE*ccgt_median_gen['HOUSTON'],
                             FOM_CCGT, CAPEX_CCGT) / 1e6
    },
    'CAPEX\n(\u00b120%)': {
        'low': calculate_npv(ccgt_median_margin['HOUSTON'], FOM_CCGT, CAPEX_CCGT*0.8) / 1e6,
        'high': calculate_npv(ccgt_median_margin['HOUSTON'], FOM_CCGT, CAPEX_CCGT*1.2) / 1e6
    },
    'WACC\n(\u00b11.5%)': {
        'low': calculate_npv(ccgt_median_margin['HOUSTON'], FOM_CCGT, CAPEX_CCGT, wacc=0.05) / 1e6,
        'high': calculate_npv(ccgt_median_margin['HOUSTON'], FOM_CCGT, CAPEX_CCGT, wacc=0.08) / 1e6
    },
    'Capacity Factor\n(\u00b110%)': {
        'low': calculate_npv(ccgt_median_margin['HOUSTON']*0.9, FOM_CCGT, CAPEX_CCGT) / 1e6,
        'high': calculate_npv(ccgt_median_margin['HOUSTON']*1.1, FOM_CCGT, CAPEX_CCGT) / 1e6
    }
}

# Create tornado data
tornado_data = []
for param, values in params.items():
    low_delta = values['low'] - baseline_ccgt_npv
    high_delta = values['high'] - baseline_ccgt_npv
    tornado_data.append({
        'parameter': param,
        'low': min(low_delta, high_delta),
        'high': max(low_delta, high_delta),
        'range': abs(high_delta - low_delta)
    })

df_tornado = pd.DataFrame(tornado_data)
df_tornado = df_tornado.sort_values('range', ascending=True)

# Plot tornado
fig, ax = plt.subplots(figsize=(10, 6))

y_pos = np.arange(len(df_tornado))
ax.barh(y_pos, df_tornado['high'], left=0, color='#E63946', alpha=0.7, label='Increase')
ax.barh(y_pos, df_tornado['low'], left=0, color='#457B9D', alpha=0.7, label='Decrease')

ax.axvline(x=0, color='black', linewidth=1)
ax.set_yticks(y_pos)
ax.set_yticklabels(df_tornado['parameter'])
ax.set_xlabel('Change in CCGT NPV ($ millions)')
ax.set_title('Tornado Chart: CCGT NPV Sensitivity to Key Parameters\n(Houston Zone, Baseline NPV = ${:.0f}M)'.format(baseline_ccgt_npv))
ax.legend(loc='lower right')

plt.tight_layout()
plt.savefig(FIG_DIR / 'fig_tornado_sensitivity.png', dpi=300, bbox_inches='tight')
plt.savefig(FIG_DIR / 'fig_tornado_sensitivity.pdf', bbox_inches='tight')
print(f"Saved: fig_tornado_sensitivity.png")
plt.close()

print("\n" + "="*70)
print("MODULE 5 COMPLETE: Sensitivity Analysis with Breakpoint Focus")
print("="*70)